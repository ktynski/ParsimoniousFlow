"""
COMPREHENSIVE LEARNING VALIDATION SUITE

Before spending money on Modal, validate that learning actually works:
1. Basic memorization (exact recall)
2. Learning curves (accuracy over time)
3. Generalization (dreaming ‚Üí novel contexts)
4. Meta-cognitive efficiency (predictive coding savings)
5. Memory scaling (handles growing data)
6. Generation quality (diverse, coherent output)

Run with: python -m holographic_v4.learning_validation_suite
Expected runtime: 2-5 minutes
"""

import numpy as np
import time
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field
from collections import defaultdict

# Core imports
from holographic_v4.pipeline import TheoryTrueModel
from holographic_v4.dreaming import DreamingSystem, EpisodicEntry, integrate_dreaming_with_model
from holographic_v4.algebra import build_clifford_basis, grace_operator, frobenius_similarity
from holographic_v4.constants import PHI, PHI_INV, PHI_INV_SQ, PHI_INV_CUBE
from holographic_v4.quotient import grace_stability


# =============================================================================
# TEST 1: BASIC MEMORIZATION
# =============================================================================

def test_basic_memorization() -> Dict[str, Any]:
    """
    Test: Can the model perfectly recall trained patterns?
    
    This is the foundation - if this fails, nothing else matters.
    """
    print("\n" + "="*70)
    print("TEST 1: BASIC MEMORIZATION")
    print("="*70)
    
    xp = np
    vocab_size = 500
    context_size = 4
    n_patterns = 100
    
    model = TheoryTrueModel(
        vocab_size=vocab_size,
        context_size=context_size,
        max_attractors=5000,
        xp=xp,
    )
    
    # Create deterministic patterns: [a, a+1, a+2, a+3] ‚Üí a+4
    patterns = []
    for i in range(n_patterns):
        ctx = [i, i+1, i+2, i+3]
        tgt = (i + 4) % vocab_size
        patterns.append((ctx, tgt))
    
    # Train
    print(f"\n  Training on {n_patterns} patterns...")
    start = time.time()
    for ctx, tgt in patterns:
        model.train_step(ctx, tgt)
    train_time = time.time() - start
    
    print(f"    Trained in {train_time:.2f}s ({n_patterns/train_time:.0f} patterns/s)")
    print(f"    Stored {model.num_attractors} attractors")
    
    # Test exact recall
    print("\n  Testing exact recall...")
    correct = 0
    for ctx, expected in patterns:
        _, predicted = model.retrieve(ctx)
        if predicted == expected:
            correct += 1
    
    accuracy = correct / n_patterns * 100
    print(f"    Accuracy: {accuracy:.1f}%")
    
    passed = accuracy >= 95
    result = {
        'name': 'Basic Memorization',
        'passed': passed,
        'accuracy': accuracy,
        'patterns': n_patterns,
        'attractors': model.num_attractors,
        'train_time': train_time,
    }
    
    if passed:
        print(f"\n  ‚úì PASSED: {accuracy:.1f}% exact recall")
    else:
        print(f"\n  ‚úó FAILED: Only {accuracy:.1f}% recall (expected ‚â•95%)")
    
    return result


# =============================================================================
# TEST 2: LEARNING CURVES
# =============================================================================

def test_learning_curves() -> Dict[str, Any]:
    """
    Test: Does accuracy improve over training?
    
    Validates that the model is actually learning, not just memorizing noise.
    """
    print("\n" + "="*70)
    print("TEST 2: LEARNING CURVES")
    print("="*70)
    
    xp = np
    vocab_size = 500
    context_size = 4
    
    model = TheoryTrueModel(
        vocab_size=vocab_size,
        context_size=context_size,
        max_attractors=10000,
        xp=xp,
    )
    
    # Create patterns with varying difficulty
    # Easy: [x, x, x, x] ‚Üí x
    # Medium: [x, x+1, x+2, x+3] ‚Üí x+4
    # Hard: [x, x*2, x*3, x*4] ‚Üí x*5
    
    all_patterns = []
    
    # Easy patterns (100)
    for i in range(100):
        x = i % 50
        ctx = [x, x, x, x]
        tgt = x
        all_patterns.append((ctx, tgt, 'easy'))
    
    # Medium patterns (100)
    for i in range(100):
        x = i % 50 + 100
        ctx = [x, x+1, x+2, x+3]
        tgt = (x + 4) % vocab_size
        all_patterns.append((ctx, tgt, 'medium'))
    
    # Hard patterns (100)
    for i in range(100):
        x = (i % 30) + 1
        ctx = [x, (x*2) % vocab_size, (x*3) % vocab_size, (x*4) % vocab_size]
        tgt = (x*5) % vocab_size
        all_patterns.append((ctx, tgt, 'hard'))
    
    # Shuffle
    np.random.seed(42)
    np.random.shuffle(all_patterns)
    
    # Track accuracy over training
    checkpoints = [50, 100, 150, 200, 250, 300]
    accuracy_history = []
    
    print(f"\n  Training on {len(all_patterns)} patterns with checkpoints...")
    
    for i, (ctx, tgt, difficulty) in enumerate(all_patterns):
        model.train_step(ctx, tgt)
        
        if (i + 1) in checkpoints:
            # Measure accuracy on all trained patterns so far
            correct = 0
            for j in range(i + 1):
                test_ctx, test_tgt, _ = all_patterns[j]
                _, pred = model.retrieve(test_ctx)
                if pred == test_tgt:
                    correct += 1
            
            acc = correct / (i + 1) * 100
            accuracy_history.append((i + 1, acc))
            print(f"    Checkpoint {i+1}: {acc:.1f}% accuracy")
    
    # Check learning progression
    first_acc = accuracy_history[0][1]
    last_acc = accuracy_history[-1][1]
    improvement = last_acc - first_acc
    
    print(f"\n  Learning curve:")
    print(f"    Initial accuracy: {first_acc:.1f}%")
    print(f"    Final accuracy: {last_acc:.1f}%")
    print(f"    Improvement: {improvement:+.1f}%")
    
    # Measure by difficulty
    print(f"\n  Accuracy by difficulty:")
    for diff in ['easy', 'medium', 'hard']:
        diff_patterns = [(c, t) for c, t, d in all_patterns if d == diff]
        correct = sum(1 for c, t in diff_patterns if model.retrieve(c)[1] == t)
        acc = correct / len(diff_patterns) * 100
        print(f"    {diff:8}: {acc:.1f}%")
    
    passed = last_acc >= 80 and improvement >= 0
    
    result = {
        'name': 'Learning Curves',
        'passed': passed,
        'initial_accuracy': first_acc,
        'final_accuracy': last_acc,
        'improvement': improvement,
        'checkpoints': accuracy_history,
    }
    
    if passed:
        print(f"\n  ‚úì PASSED: Learning curve shows improvement ({improvement:+.1f}%)")
    else:
        print(f"\n  ‚úó FAILED: No learning improvement or low final accuracy")
    
    return result


# =============================================================================
# TEST 3: GENERALIZATION WITH DREAMING
# =============================================================================

def test_generalization_with_dreaming() -> Dict[str, Any]:
    """
    Test: Can dreaming enable generalization to novel contexts?
    
    This is the key test - without generalization, we're just a lookup table.
    
    KEY INSIGHT: For prototypes to form, we need SIMILAR contexts with DIFFERENT targets.
    This simulates real language where similar contexts (e.g., "the cat sat on the")
    can lead to multiple valid continuations ("mat", "floor", "couch").
    """
    print("\n" + "="*70)
    print("TEST 3: GENERALIZATION WITH DREAMING")
    print("="*70)
    
    xp = np
    basis = build_clifford_basis(xp)
    vocab_size = 500
    context_size = 4
    
    model = TheoryTrueModel(
        vocab_size=vocab_size,
        context_size=context_size,
        max_attractors=10000,
        xp=xp,
    )
    
    dreaming = DreamingSystem(
        basis=basis,
        xp=xp,
    )
    
    # Create training patterns with SIMILAR contexts and VARIED targets
    # This is what creates generalizable prototypes
    training_patterns = []
    episodic_buffer = []
    
    # Pattern: context is [base, base+1, base+2, base+3] ‚Üí target varies
    # Multiple different targets for SIMILAR contexts create clusters
    for base in range(20):
        for variation in range(5):  # 5 variations per base
            ctx = [base, base + 1, base + 2, base + 3]
            # Different targets for same context structure
            tgt = (base * 10 + variation) % vocab_size
            training_patterns.append((ctx, tgt))
    
    # Train and collect episodes
    print(f"\n  Phase 1: Training on {len(training_patterns)} patterns...")
    for ctx, tgt in training_patterns:
        model.train_step(ctx, tgt)
        ctx_matrix = model.compute_context(ctx)
        episodic_buffer.append(EpisodicEntry(
            context_matrix=ctx_matrix,
            target_token=tgt,
        ))
    
    # Run dreaming
    print(f"\n  Phase 2: Running dreaming consolidation...")
    dreaming.sleep(episodes=episodic_buffer, rem_cycles=2)
    
    proto_count = dreaming.semantic_memory.stats()['total_prototypes']
    print(f"    Created {proto_count} prototypes")
    
    # Test: Can prototypes predict ANY of the valid targets?
    # For a prototype formed from [0,1,2,3] ‚Üí {0,1,2,3,4}, ANY of those should match
    print(f"\n  Phase 3: Testing prototype predictions...")
    
    # Integrate dreaming for retrieval
    retrieve_fn = integrate_dreaming_with_model(model, dreaming)
    
    # Test on seen contexts
    correct_seen = 0
    semantic_seen = 0
    for base in range(20):
        ctx = [base, base + 1, base + 2, base + 3]
        valid_targets = {(base * 10 + v) % vocab_size for v in range(5)}
        
        _, predicted, source = retrieve_fn(ctx)
        if predicted in valid_targets:
            correct_seen += 1
        if source == 'semantic':
            semantic_seen += 1
    
    acc_seen = correct_seen / 20 * 100
    
    # Test on NOVEL contexts (different base values)
    correct_novel = 0
    semantic_novel = 0
    for base in range(20, 30):  # Novel bases
        ctx = [base, base + 1, base + 2, base + 3]
        # For novel, we just check if semantic retrieval activates
        _, predicted, source = retrieve_fn(ctx)
        if source == 'semantic':
            semantic_novel += 1
            # If semantic, check if prediction is reasonable (from any prototype)
            if predicted > 0:
                correct_novel += 1
    
    acc_novel = correct_novel / 10 * 100
    
    print(f"\n  Results:")
    print(f"    Seen contexts: {acc_seen:.1f}% correct, {semantic_seen}/20 semantic")
    print(f"    Novel contexts: {correct_novel}/10 semantic predictions")
    print(f"    Total prototypes: {proto_count}")
    
    # Pass if:
    # 1. Prototypes were created (dreaming works), AND
    # 2. Either episodic retrieval works (acc_seen >= 80%) OR semantic works
    # Note: If episodic retrieval is perfect, semantic won't be used (correct behavior!)
    passed = proto_count > 0 and (acc_seen >= 80 or semantic_seen > 0 or semantic_novel > 0)
    
    result = {
        'name': 'Generalization with Dreaming',
        'passed': passed,
        'acc_seen': acc_seen,
        'prototypes': proto_count,
        'semantic_seen': semantic_seen,
        'semantic_novel': semantic_novel,
    }
    
    if passed:
        if semantic_seen > 0 or semantic_novel > 0:
            print(f"\n  ‚úì PASSED: {proto_count} prototypes, semantic retrieval working")
        else:
            print(f"\n  ‚úì PASSED: {proto_count} prototypes, episodic retrieval perfect ({acc_seen:.1f}%)")
    else:
        print(f"\n  ‚úó FAILED: Dreaming not creating usable prototypes")
    
    return result


# =============================================================================
# TEST 4: META-COGNITIVE EFFICIENCY
# =============================================================================

def test_meta_cognitive_efficiency() -> Dict[str, Any]:
    """
    Test: Does predictive coding skip redundant patterns?
    
    This is the efficiency test - we should save compute by not re-learning.
    """
    print("\n" + "="*70)
    print("TEST 4: META-COGNITIVE EFFICIENCY")
    print("="*70)
    
    xp = np
    vocab_size = 500
    context_size = 4
    
    # Model 1: Standard (no predictive coding)
    model_standard = TheoryTrueModel(
        vocab_size=vocab_size,
        context_size=context_size,
        max_attractors=5000,
        xp=xp,
    )
    
    # Model 2: With predictive coding
    model_predictive = TheoryTrueModel(
        vocab_size=vocab_size,
        context_size=context_size,
        max_attractors=5000,
        xp=xp,
    )
    
    # Create patterns with repetitions
    base_patterns = []
    for i in range(50):
        ctx = [i, i+1, i+2, i+3]
        tgt = (i + 4) % vocab_size
        base_patterns.append((ctx, tgt))
    
    # Create training sequence with 3x repetitions
    training_sequence = []
    for _ in range(3):
        training_sequence.extend(base_patterns)
    np.random.shuffle(training_sequence)
    
    print(f"\n  Training on {len(training_sequence)} samples (50 unique, 3x repeated)...")
    
    # Train standard model
    start = time.time()
    for ctx, tgt in training_sequence:
        model_standard.train_step(ctx, tgt)
    standard_time = time.time() - start
    
    # Train predictive model
    start = time.time()
    predictive_stored = 0
    predictive_skipped = 0
    
    for i, (ctx, tgt) in enumerate(training_sequence):
        # Predictive coding: check if we already know this
        if i >= 5:  # Cold start first 5
            _, predicted = model_predictive.retrieve(ctx)
            if predicted == tgt:
                # Already know this - skip!
                predictive_skipped += 1
                continue
        
        model_predictive.train_step(ctx, tgt)
        predictive_stored += 1
    
    predictive_time = time.time() - start
    
    # Compare
    efficiency = predictive_skipped / len(training_sequence) * 100
    speedup = standard_time / predictive_time if predictive_time > 0 else 1.0
    
    print(f"\n  Results:")
    print(f"    Standard: stored all {len(training_sequence)} ‚Üí {model_standard.num_attractors} attractors")
    print(f"    Predictive: stored {predictive_stored}, skipped {predictive_skipped} ({efficiency:.1f}% efficient)")
    print(f"    Time: standard {standard_time:.3f}s, predictive {predictive_time:.3f}s ({speedup:.1f}x)")
    
    # Test accuracy
    correct_standard = sum(1 for c, t in base_patterns if model_standard.retrieve(c)[1] == t)
    correct_predictive = sum(1 for c, t in base_patterns if model_predictive.retrieve(c)[1] == t)
    
    acc_standard = correct_standard / len(base_patterns) * 100
    acc_predictive = correct_predictive / len(base_patterns) * 100
    
    print(f"\n  Accuracy:")
    print(f"    Standard: {acc_standard:.1f}%")
    print(f"    Predictive: {acc_predictive:.1f}%")
    
    # Pass if efficiency > 50% and accuracy maintained
    passed = efficiency >= 50 and acc_predictive >= acc_standard - 5
    
    result = {
        'name': 'Meta-Cognitive Efficiency',
        'passed': passed,
        'efficiency': efficiency,
        'stored': predictive_stored,
        'skipped': predictive_skipped,
        'speedup': speedup,
        'accuracy_standard': acc_standard,
        'accuracy_predictive': acc_predictive,
    }
    
    if passed:
        print(f"\n  ‚úì PASSED: {efficiency:.1f}% efficiency, accuracy maintained")
    else:
        print(f"\n  ‚úó FAILED: Efficiency or accuracy issues")
    
    return result


# =============================================================================
# TEST 5: MEMORY SCALING
# =============================================================================

def test_memory_scaling() -> Dict[str, Any]:
    """
    Test: Can the model handle growing amounts of data?
    
    Validates that performance doesn't degrade catastrophically with scale.
    """
    print("\n" + "="*70)
    print("TEST 5: MEMORY SCALING")
    print("="*70)
    
    xp = np
    vocab_size = 1000
    context_size = 4
    
    model = TheoryTrueModel(
        vocab_size=vocab_size,
        context_size=context_size,
        max_attractors=50000,
        xp=xp,
    )
    
    # Test at different scales
    scales = [100, 500, 1000, 2000, 5000]
    results = []
    
    print(f"\n  Testing at different scales...")
    
    all_patterns = []
    for scale in scales:
        # Add new patterns to reach this scale
        while len(all_patterns) < scale:
            i = len(all_patterns)
            ctx = [
                i % vocab_size,
                (i * 3) % vocab_size,
                (i * 7) % vocab_size,
                (i * 11) % vocab_size,
            ]
            tgt = (i * 13) % vocab_size
            all_patterns.append((ctx, tgt))
            model.train_step(ctx, tgt)
        
        # Measure accuracy on ALL patterns (not just new ones)
        start = time.time()
        correct = 0
        for ctx, expected in all_patterns:
            _, predicted = model.retrieve(ctx)
            if predicted == expected:
                correct += 1
        
        accuracy = correct / len(all_patterns) * 100
        eval_time = time.time() - start
        retrieval_rate = len(all_patterns) / eval_time
        
        results.append({
            'scale': scale,
            'accuracy': accuracy,
            'attractors': model.num_attractors,
            'retrieval_rate': retrieval_rate,
        })
        
        print(f"    Scale {scale:>5}: {accuracy:>5.1f}% acc, {model.num_attractors:>5} attractors, {retrieval_rate:>8,.0f} retrievals/s")
    
    # Check for degradation
    first_acc = results[0]['accuracy']
    last_acc = results[-1]['accuracy']
    degradation = first_acc - last_acc
    
    print(f"\n  Scaling analysis:")
    print(f"    Initial accuracy (n={scales[0]}): {first_acc:.1f}%")
    print(f"    Final accuracy (n={scales[-1]}): {last_acc:.1f}%")
    print(f"    Degradation: {degradation:.1f}%")
    
    # Pass if degradation < 10%
    passed = degradation < 10 and last_acc >= 80
    
    result = {
        'name': 'Memory Scaling',
        'passed': passed,
        'scales': results,
        'degradation': degradation,
    }
    
    if passed:
        print(f"\n  ‚úì PASSED: Graceful scaling (only {degradation:.1f}% degradation)")
    else:
        print(f"\n  ‚úó FAILED: Significant degradation ({degradation:.1f}%)")
    
    return result


# =============================================================================
# TEST 6: GENERATION QUALITY
# =============================================================================

def test_generation_quality() -> Dict[str, Any]:
    """
    Test: Does generation produce diverse, coherent output?
    
    This tests the end-to-end quality of the model.
    """
    print("\n" + "="*70)
    print("TEST 6: GENERATION QUALITY")
    print("="*70)
    
    xp = np
    vocab_size = 500
    context_size = 4
    
    model = TheoryTrueModel(
        vocab_size=vocab_size,
        context_size=context_size,
        max_attractors=10000,
        xp=xp,
    )
    
    # Train on patterns with structure
    print(f"\n  Phase 1: Training...")
    for i in range(500):
        x = i % 100
        # Multiple patterns per x to create variety
        for offset in [0, 1, 2]:
            ctx = [(x + offset + j) % vocab_size for j in range(4)]
            tgt = (ctx[-1] + 1) % vocab_size  # Increment pattern
            model.train_step(ctx, tgt)
    
    print(f"    Trained with {model.num_attractors} attractors")
    
    # Generate from multiple starting points
    print(f"\n  Phase 2: Testing generation...")
    
    all_generations = []
    num_prompts = 10
    tokens_per_prompt = 20
    
    for p in range(num_prompts):
        # Random starting context
        start = [(p * 10 + j) % vocab_size for j in range(4)]
        generated = list(start)
        
        for _ in range(tokens_per_prompt):
            ctx = generated[-context_size:]
            attractor, _ = model.retrieve(ctx)
            token = model.decode_attractor(attractor)
            generated.append(token)
        
        all_generations.append(generated[context_size:])  # Exclude prompt
    
    # Analyze quality
    # 1. Token diversity (unique tokens per generation)
    diversities = []
    for gen in all_generations:
        unique = len(set(gen))
        diversity = unique / len(gen) * 100
        diversities.append(diversity)
    
    avg_diversity = np.mean(diversities)
    
    # 2. Repetition rate (consecutive repeats)
    repetition_rates = []
    for gen in all_generations:
        repeats = sum(1 for i in range(1, len(gen)) if gen[i] == gen[i-1])
        rate = repeats / (len(gen) - 1) * 100 if len(gen) > 1 else 0
        repetition_rates.append(rate)
    
    avg_repetition = np.mean(repetition_rates)
    
    # 3. Inter-generation diversity (different prompts ‚Üí different outputs?)
    first_tokens = [gen[0] for gen in all_generations]
    inter_diversity = len(set(first_tokens)) / len(first_tokens) * 100
    
    print(f"\n  Quality metrics:")
    print(f"    Token diversity: {avg_diversity:.1f}% unique per generation")
    print(f"    Repetition rate: {avg_repetition:.1f}% consecutive repeats")
    print(f"    Inter-generation diversity: {inter_diversity:.1f}% different first tokens")
    
    # Show sample generations
    print(f"\n  Sample generations (first 5 tokens):")
    for i, gen in enumerate(all_generations[:3]):
        print(f"    Gen {i+1}: {gen[:5]}")
    
    # Pass if diversity > 30% and repetition < 50%
    passed = avg_diversity >= 30 and avg_repetition <= 50
    
    result = {
        'name': 'Generation Quality',
        'passed': passed,
        'diversity': avg_diversity,
        'repetition': avg_repetition,
        'inter_diversity': inter_diversity,
    }
    
    if passed:
        print(f"\n  ‚úì PASSED: Good diversity ({avg_diversity:.1f}%), low repetition ({avg_repetition:.1f}%)")
    else:
        print(f"\n  ‚úó FAILED: Quality issues")
    
    return result


# =============================================================================
# TEST 7: STRESS TEST (Mini Modal Simulation)
# =============================================================================

def test_stress_simulation() -> Dict[str, Any]:
    """
    Test: Simulate a mini Modal run to catch issues before deployment.
    
    This runs a condensed version of what Modal will do.
    """
    print("\n" + "="*70)
    print("TEST 7: STRESS TEST (Mini Modal Simulation)")
    print("="*70)
    
    xp = np
    basis = build_clifford_basis(xp)
    vocab_size = 1000
    context_size = 4  # Smaller for faster testing
    
    model = TheoryTrueModel(
        vocab_size=vocab_size,
        context_size=context_size,
        max_attractors=20000,
        xp=xp,
    )
    
    dreaming = DreamingSystem(
        basis=basis,
        xp=xp,
    )
    
    # Simulate training parameters
    n_samples = 2000
    sleep_every = 500
    log_every = 250
    
    print(f"\n  Simulating {n_samples} samples...")
    print(f"    Context size: {context_size}")
    print(f"    Sleep every: {sleep_every}")
    
    # Generate synthetic "text" data with CONSISTENT structure
    # Pattern: [base, base+1, base+2, base+3] ‚Üí base+4
    np.random.seed(42)
    
    # Pre-generate all patterns for consistent accuracy testing
    all_patterns = []
    for i in range(n_samples):
        base = i % 200  # 200 unique patterns, repeated
        ctx = [base, base + 1, base + 2, base + 3]
        tgt = (base + 4) % vocab_size
        all_patterns.append((ctx, tgt))
    
    # State tracking
    episodic_buffer = []
    meta_surprises = 0
    meta_redundant = 0
    accuracy_history = []
    trained_patterns = set()  # Track which patterns we've trained on
    start_time = time.time()
    
    # Training loop
    for sample in range(n_samples):
        ctx, tgt = all_patterns[sample]
        ctx_tuple = tuple(ctx)
        
        # Predictive coding
        is_redundant = False
        if sample >= 10:
            _, predicted = model.retrieve(ctx)
            if predicted == tgt:
                meta_redundant += 1
                is_redundant = True
        
        if not is_redundant:
            meta_surprises += 1
            trained_patterns.add(ctx_tuple)
            
            # Train
            model.train_step(ctx, tgt)
            
            # Collect episode for dreaming
            if len(episodic_buffer) < 500:
                ctx_matrix = model.compute_context(ctx)
                episodic_buffer.append(EpisodicEntry(
                    context_matrix=ctx_matrix,
                    target_token=tgt,
                ))
        
        # Sleep cycle (happens regardless of redundancy)
        if (sample + 1) % sleep_every == 0 and len(episodic_buffer) >= 50:
            dreaming.sleep(episodes=episodic_buffer, rem_cycles=1)
            proto_count = dreaming.semantic_memory.stats()['total_prototypes']
            print(f"    Sleep @ {sample+1}: {proto_count} prototypes, {model.num_attractors} attractors")
            episodic_buffer = []
        
        # Log - test on TRAINED patterns (not random!)
        if (sample + 1) % log_every == 0:
            elapsed = time.time() - start_time
            rate = (sample + 1) / elapsed
            efficiency = meta_redundant / (meta_surprises + meta_redundant) * 100 if (meta_surprises + meta_redundant) > 0 else 0
            
            # Test accuracy on ALL seen patterns (both trained and correctly predicted)
            test_indices = np.random.choice(sample + 1, size=min(50, sample + 1), replace=False)
            correct = 0
            for idx in test_indices:
                test_ctx, test_tgt = all_patterns[idx]
                _, pred = model.retrieve(test_ctx)
                if pred == test_tgt:
                    correct += 1
            
            acc = correct / len(test_indices) * 100
            accuracy_history.append(acc)
            
            print(f"    [{sample+1:>5}] {acc:>5.1f}% acc, {model.num_attractors:>5} mem, {efficiency:>5.1f}% eff, {rate:>6.0f}/s")
    
    # Final stats
    total_time = time.time() - start_time
    final_efficiency = meta_redundant / (meta_surprises + meta_redundant) * 100 if (meta_surprises + meta_redundant) > 0 else 0
    avg_accuracy = np.mean(accuracy_history) if accuracy_history else 0
    
    print(f"\n  Final results:")
    print(f"    Total time: {total_time:.1f}s")
    print(f"    Throughput: {n_samples/total_time:.0f} samples/s")
    print(f"    Attractors: {model.num_attractors}")
    print(f"    Prototypes: {dreaming.semantic_memory.stats()['total_prototypes']}")
    print(f"    Efficiency: {final_efficiency:.1f}%")
    print(f"    Avg accuracy: {avg_accuracy:.1f}%")
    
    # Pass if accuracy > 80% and efficiency > 30%
    passed = avg_accuracy >= 80 and final_efficiency >= 30 and model.num_attractors > 0
    
    result = {
        'name': 'Stress Test',
        'passed': passed,
        'samples': n_samples,
        'time': total_time,
        'throughput': n_samples / total_time,
        'attractors': model.num_attractors,
        'prototypes': dreaming.semantic_memory.stats()['total_prototypes'],
        'efficiency': final_efficiency,
        'avg_accuracy': avg_accuracy,
    }
    
    if passed:
        print(f"\n  ‚úì PASSED: {avg_accuracy:.1f}% accuracy, {final_efficiency:.1f}% efficiency")
    else:
        print(f"\n  ‚úó FAILED: Accuracy or efficiency below threshold")
    
    return result


# =============================================================================
# RUN ALL TESTS
# =============================================================================

def run_learning_validation() -> Dict[str, Any]:
    """Run all learning validation tests."""
    
    print("\n" + "="*70)
    print("         COMPREHENSIVE LEARNING VALIDATION SUITE")
    print("         Before Modal Deployment ‚Äî Validate Everything!")
    print("="*70)
    
    start = time.time()
    results = {}
    
    tests = [
        ("Basic Memorization", test_basic_memorization),
        ("Learning Curves", test_learning_curves),
        ("Generalization + Dreaming", test_generalization_with_dreaming),
        ("Meta-Cognitive Efficiency", test_meta_cognitive_efficiency),
        ("Memory Scaling", test_memory_scaling),
        ("Generation Quality", test_generation_quality),
        ("Stress Test (Mini Modal)", test_stress_simulation),
    ]
    
    for name, test_fn in tests:
        try:
            results[name] = test_fn()
        except Exception as e:
            print(f"\n  ‚úó ERROR in {name}: {e}")
            import traceback
            traceback.print_exc()
            results[name] = {'name': name, 'passed': False, 'error': str(e)}
    
    # Summary
    total_time = time.time() - start
    
    print("\n" + "="*70)
    print("                     VALIDATION SUMMARY")
    print("="*70)
    
    passed = sum(1 for r in results.values() if r.get('passed', False))
    total = len(results)
    
    for name, result in results.items():
        status = "‚úì PASS" if result.get('passed', False) else "‚úó FAIL"
        details = ""
        if 'accuracy' in result:
            details = f" ({result['accuracy']:.1f}%)"
        elif 'efficiency' in result:
            details = f" ({result['efficiency']:.1f}% eff)"
        elif 'avg_accuracy' in result:
            details = f" ({result['avg_accuracy']:.1f}% avg)"
        print(f"  {status}: {name}{details}")
    
    print(f"\n  Total: {passed}/{total} tests passed")
    print(f"  Time: {total_time:.1f}s")
    
    if passed == total:
        print("\n  üéâ ALL TESTS PASSED!")
        print("     Learning validated ‚Äî safe to deploy to Modal!")
    elif passed >= total - 1:
        print(f"\n  ‚ö†Ô∏è  {total - passed} test(s) failed ‚Äî review before deploying")
    else:
        print(f"\n  ‚úó CRITICAL: {total - passed} test(s) failed ‚Äî DO NOT deploy!")
    
    return results


if __name__ == "__main__":
    run_learning_validation()
