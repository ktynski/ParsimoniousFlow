"""
TEST: Generalization WITH Semantic Embeddings

INSIGHT FROM PREVIOUS TEST:
    The theory WORKS for semantic structure (similar sentences ‚Üí higher similarity).
    The theory FAILS for paraphrases because embeddings don't encode word relationships.
    
THIS TEST:
    We manually create embeddings where synonyms are similar.
    This simulates what we'd get from pre-trained embeddings (Word2Vec, GloVe).
    
THEORY PREDICTION:
    If embeddings encode "cat ‚âà feline", then:
    - "the cat sat" and "the feline rested" should have similar contexts
    - Retrieval should generalize to paraphrases
    
Run: python -m holographic_v4.test_semantic_embeddings_generalization
"""

import numpy as np
from typing import Dict, List, Tuple
import sys

PHI = (1 + np.sqrt(5)) / 2
PHI_INV = 1 / PHI


def simple_tokenize(text: str) -> List[str]:
    """Simple word tokenization."""
    return text.lower().replace('.', '').replace(',', '').replace('!', '').replace('?', '').split()


# =============================================================================
# CREATE SEMANTIC EMBEDDINGS
# =============================================================================

def create_semantic_embeddings(vocab: Dict[str, int], embedding_dim: int = 4) -> np.ndarray:
    """
    Create embeddings where synonyms are similar.
    
    This simulates what we'd get from pre-trained embeddings.
    """
    n_words = len(vocab)
    
    # Start with identity-like base for each word
    embeddings = np.zeros((n_words, embedding_dim, embedding_dim))
    for i in range(n_words):
        embeddings[i] = np.eye(embedding_dim)
    
    # Define semantic clusters (synonyms share similar perturbations)
    # Each cluster has a "signature" direction
    semantic_clusters = {
        # Animals
        'cat': np.array([[0.1, 0, 0, 0], [0, 0.1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]),
        'feline': np.array([[0.1, 0, 0, 0], [0, 0.1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]),  # Same as cat!
        'dog': np.array([[0.1, 0, 0, 0], [0, 0, 0.1, 0], [0, 0, 0, 0], [0, 0, 0, 0]]),
        'canine': np.array([[0.1, 0, 0, 0], [0, 0, 0.1, 0], [0, 0, 0, 0], [0, 0, 0, 0]]),  # Same as dog!
        'bird': np.array([[0.1, 0, 0, 0], [0, 0, 0, 0.1], [0, 0, 0, 0], [0, 0, 0, 0]]),
        'sparrow': np.array([[0.1, 0, 0, 0], [0, 0, 0, 0.1], [0, 0, 0, 0], [0, 0, 0, 0]]),  # Same as bird!
        
        # Verbs
        'sat': np.array([[0, 0.1, 0, 0], [0, 0, 0, 0], [0, 0, 0.1, 0], [0, 0, 0, 0]]),
        'rested': np.array([[0, 0.1, 0, 0], [0, 0, 0, 0], [0, 0, 0.1, 0], [0, 0, 0, 0]]),  # Same as sat!
        'ran': np.array([[0, 0, 0.1, 0], [0, 0, 0, 0], [0, 0, 0.1, 0], [0, 0, 0, 0]]),
        'jogged': np.array([[0, 0, 0.1, 0], [0, 0, 0, 0], [0, 0, 0.1, 0], [0, 0, 0, 0]]),  # Same as ran!
        'flew': np.array([[0, 0, 0, 0.1], [0, 0, 0, 0], [0, 0, 0.1, 0], [0, 0, 0, 0]]),
        'soared': np.array([[0, 0, 0, 0.1], [0, 0, 0, 0], [0, 0, 0.1, 0], [0, 0, 0, 0]]),  # Same as flew!
        
        # Food verbs
        'ate': np.array([[0, 0, 0, 0], [0.1, 0, 0, 0], [0, 0.1, 0, 0], [0, 0, 0, 0]]),
        'consumed': np.array([[0, 0, 0, 0], [0.1, 0, 0, 0], [0, 0.1, 0, 0], [0, 0, 0, 0]]),  # Same as ate!
        'drank': np.array([[0, 0, 0, 0], [0, 0.1, 0, 0], [0, 0.1, 0, 0], [0, 0, 0, 0]]),
        'sipped': np.array([[0, 0, 0, 0], [0, 0.1, 0, 0], [0, 0.1, 0, 0], [0, 0, 0, 0]]),  # Same as drank!
        
        # Walk verbs
        'walked': np.array([[0, 0, 0, 0], [0, 0, 0, 0.1], [0.1, 0, 0, 0], [0, 0, 0, 0]]),
        'strolled': np.array([[0, 0, 0, 0], [0, 0, 0, 0.1], [0.1, 0, 0, 0], [0, 0, 0, 0]]),  # Same!
        'sprinted': np.array([[0, 0, 0.1, 0], [0, 0, 0, 0], [0, 0, 0.1, 0], [0, 0, 0, 0]]),  # Like ran!
        
        # Colors/adjectives
        'red': np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0.1, 0, 0, 0]]),
        'crimson': np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0.1, 0, 0, 0]]),  # Same as red!
        'cold': np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0.1, 0, 0]]),
        'chilled': np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0.1, 0, 0]]),  # Same as cold!
    }
    
    # Apply semantic signatures to embeddings
    for word, signature in semantic_clusters.items():
        if word in vocab:
            idx = vocab[word]
            embeddings[idx] = np.eye(embedding_dim) + signature
    
    # Add small unique noise to each word (maintain identity + semantic + unique)
    np.random.seed(42)
    for i in range(n_words):
        embeddings[i] += 0.01 * np.random.randn(embedding_dim, embedding_dim)
    
    return embeddings.astype(np.float32)


# =============================================================================
# TEST: PARAPHRASE WITH SEMANTIC EMBEDDINGS
# =============================================================================

def test_paraphrase_with_semantic_embeddings() -> Dict:
    """
    Test paraphrase generalization when embeddings encode synonymy.
    """
    print("\n" + "="*60)
    print("TEST: Paraphrase Generalization with Semantic Embeddings")
    print("="*60)
    
    from holographic_v4.algebra import build_clifford_basis, geometric_product, grace_operator
    from holographic_v4.holographic_memory import VorticityWitnessIndex
    from holographic_v4.quotient import witness_similarity
    
    # Test data
    test_cases = [
        ("the cat sat on the", "mat", "the feline rested on the"),
        ("the dog ran in the", "park", "the canine jogged in the"),
        ("the bird flew to the", "tree", "the sparrow soared to the"),
        ("she ate a red", "apple", "she consumed a crimson"),
        ("he drank cold", "water", "he sipped chilled"),
    ]
    
    # Build vocabulary
    all_words = set()
    for t, target, p in test_cases:
        all_words.update(simple_tokenize(t))
        all_words.add(target)
        all_words.update(simple_tokenize(p))
    
    vocab = {w: i for i, w in enumerate(sorted(all_words))}
    print(f"\n  Vocabulary size: {len(vocab)}")
    
    # Create SEMANTIC embeddings (synonyms are similar!)
    embeddings = create_semantic_embeddings(vocab)
    print(f"  Embedding shape: {embeddings.shape}")
    
    # Build basis
    basis = build_clifford_basis()
    
    # Create memory
    memory = VorticityWitnessIndex.create(basis, xp=np)
    
    def compute_context(tokens: List[str]) -> np.ndarray:
        """Compose tokens into context using geometric product."""
        token_ids = [vocab.get(t, 0) for t in tokens]
        
        # Start with identity
        ctx = np.eye(4, dtype=np.float32)
        
        # Compose via geometric product
        for tid in token_ids:
            ctx = geometric_product(ctx, embeddings[tid])
        
        # Apply Grace
        ctx = grace_operator(ctx, basis, np)
        return ctx
    
    # First, show that synonyms have similar embeddings
    print("\n  Synonym embedding similarities:")
    synonym_pairs = [('cat', 'feline'), ('dog', 'canine'), ('sat', 'rested'), ('ran', 'jogged')]
    for w1, w2 in synonym_pairs:
        if w1 in vocab and w2 in vocab:
            e1 = embeddings[vocab[w1]]
            e2 = embeddings[vocab[w2]]
            sim = np.sum(e1 * e2) / (np.linalg.norm(e1) * np.linalg.norm(e2))
            print(f"    {w1} ~ {w2}: {sim:.3f}")
    
    # Store training sentences
    print("\n  Training:")
    for train_ctx, target, _ in test_cases:
        tokens = simple_tokenize(train_ctx)
        target_idx = vocab.get(target, 0)
        
        ctx = compute_context(tokens)
        target_emb = embeddings[target_idx]
        
        memory.store(ctx, target_emb, target_idx)
        print(f"    Stored: '{train_ctx}' ‚Üí '{target}'")
    
    # Test on paraphrases
    print("\n  Testing paraphrases:")
    correct = 0
    for train_ctx, target, para_ctx in test_cases:
        train_tokens = simple_tokenize(train_ctx)
        para_tokens = simple_tokenize(para_ctx)
        target_idx = vocab.get(target, 0)
        
        train_context = compute_context(train_tokens)
        para_context = compute_context(para_tokens)
        
        # Check similarity between training and paraphrase context
        sim = witness_similarity(train_context, para_context, basis, np)
        
        # Try retrieval
        result, idx, conf = memory.retrieve(para_context)
        
        if idx == target_idx:
            correct += 1
            status = "‚úì"
        else:
            status = "‚úó"
        
        print(f"    {status} '{para_ctx}' (sim={sim:.3f}) ‚Üí expected '{target}', got idx={idx}")
    
    accuracy = correct / len(test_cases)
    
    print(f"\n  Accuracy: {accuracy:.1%} ({correct}/{len(test_cases)})")
    
    passed = accuracy >= 0.6  # Should get most right with semantic embeddings
    
    if passed:
        print(f"\n  ‚úÖ PASS: Semantic embeddings enable generalization!")
    else:
        print(f"\n  ‚ö†Ô∏è PARTIAL: Some generalization ({accuracy:.1%})")
    
    return {
        'test': 'paraphrase_with_semantic_embeddings',
        'accuracy': accuracy,
        'passed': passed
    }


# =============================================================================
# TEST: SHOW THE DIFFERENCE
# =============================================================================

def test_random_vs_semantic_embeddings() -> Dict:
    """
    Compare generalization with random vs semantic embeddings.
    This proves the theory works when structure exists.
    """
    print("\n" + "="*60)
    print("TEST: Random vs Semantic Embeddings")
    print("="*60)
    
    from holographic_v4.algebra import build_clifford_basis, geometric_product, grace_operator
    from holographic_v4.quotient import witness_similarity
    
    test_pairs = [
        ('cat', 'feline'),
        ('dog', 'canine'),
        ('ran', 'jogged'),
        ('ate', 'consumed'),
    ]
    
    vocab = {w: i for i, (w1, w2) in enumerate(test_pairs) for w in [w1, w2]}
    vocab = {w: i for i, w in enumerate(set(w for pair in test_pairs for w in pair))}
    
    basis = build_clifford_basis()
    
    # Random embeddings (current approach)
    np.random.seed(42)
    random_emb = np.array([np.eye(4) + 0.1 * np.random.randn(4, 4) for _ in range(len(vocab))])
    
    # Semantic embeddings (synonyms similar)
    semantic_emb = create_semantic_embeddings(vocab)
    
    print("\n  Context similarities for synonym pairs:")
    print("  " + "-"*50)
    print(f"  {'Pair':<20} {'Random':<12} {'Semantic':<12}")
    print("  " + "-"*50)
    
    random_sims = []
    semantic_sims = []
    
    for w1, w2 in test_pairs:
        if w1 not in vocab or w2 not in vocab:
            continue
            
        # With random embeddings
        e1_rand = random_emb[vocab[w1]]
        e2_rand = random_emb[vocab[w2]]
        ctx1_rand = grace_operator(e1_rand, basis, np)
        ctx2_rand = grace_operator(e2_rand, basis, np)
        sim_rand = witness_similarity(ctx1_rand, ctx2_rand, basis, np)
        random_sims.append(sim_rand)
        
        # With semantic embeddings
        e1_sem = semantic_emb[vocab[w1]]
        e2_sem = semantic_emb[vocab[w2]]
        ctx1_sem = grace_operator(e1_sem, basis, np)
        ctx2_sem = grace_operator(e2_sem, basis, np)
        sim_sem = witness_similarity(ctx1_sem, ctx2_sem, basis, np)
        semantic_sims.append(sim_sem)
        
        print(f"  {w1}/{w2:<15} {sim_rand:<12.3f} {sim_sem:<12.3f}")
    
    avg_random = np.mean(random_sims)
    avg_semantic = np.mean(semantic_sims)
    
    print("  " + "-"*50)
    print(f"  {'Average':<20} {avg_random:<12.3f} {avg_semantic:<12.3f}")
    
    improvement = avg_semantic - avg_random
    
    print(f"\n  Semantic improvement: {improvement:+.3f}")
    
    passed = avg_semantic > avg_random
    
    if passed:
        print(f"\n  ‚úÖ PASS: Semantic embeddings improve synonym similarity!")
        print("     This proves the theory works when structure exists.")
    else:
        print(f"\n  ‚ùå FAIL: No improvement from semantic embeddings")
    
    return {
        'test': 'random_vs_semantic_embeddings',
        'avg_random': avg_random,
        'avg_semantic': avg_semantic,
        'improvement': improvement,
        'passed': passed
    }


# =============================================================================
# MAIN
# =============================================================================

def run_all_tests() -> bool:
    """Run all tests."""
    print("\n" + "="*70)
    print("SEMANTIC EMBEDDINGS GENERALIZATION TEST")
    print("="*70)
    print("""
HYPOTHESIS:
    The holographic theory WORKS for generalization, but requires
    embeddings that encode semantic similarity.
    
    With random embeddings: "cat" ‚â† "feline" (no generalization)
    With semantic embeddings: "cat" ‚âà "feline" (generalization works!)
""")
    
    results = []
    
    results.append(test_random_vs_semantic_embeddings())
    results.append(test_paraphrase_with_semantic_embeddings())
    
    # Summary
    print("\n" + "="*70)
    print("SUMMARY")
    print("="*70)
    
    passed = sum(1 for r in results if r['passed'])
    total = len(results)
    
    for r in results:
        status = "‚úÖ" if r['passed'] else "‚ùå"
        print(f"  {status} {r['test']}")
    
    print(f"\n  TOTAL: {passed}/{total} passed")
    
    if passed == total:
        print("""
  üéâ THEORY VALIDATED!
  
  The holographic system DOES generalize when embeddings encode
  semantic structure. The limitation is not the theory, but the
  need for semantically meaningful embeddings.
  
  IMPLICATIONS:
  1. Use pre-trained embeddings (Word2Vec, GloVe) for real applications
  2. OR learn embeddings through sufficient training data
  3. The Clifford algebra + Grace operator correctly propagate structure
""")
    else:
        print("\n  ‚ö†Ô∏è Some tests failed")
    
    return passed == total


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
