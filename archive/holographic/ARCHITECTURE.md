# Holographic Language Model â€” Architecture v3.0

## Quick Reference: The Breakthrough

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPOSITIONAL EMBEDDINGS (v3.0)                          â”‚
â”‚                                                                             â”‚
â”‚   THE INSIGHT:                                                              â”‚
â”‚       We were using Clifford algebra at the WRONG level.                   â”‚
â”‚                                                                             â”‚
â”‚   WRONG (atomic):   word = random 4Ã—4 matrix                               â”‚
â”‚                     â†’ Learns co-occurrence, not semantics                   â”‚
â”‚                     â†’ Separation: 0.06                                      â”‚
â”‚                                                                             â”‚
â”‚   RIGHT (compositional):  word = I + Î£áµ¢ Î±áµ¢(word) Â· fáµ¢                      â”‚
â”‚                     where fáµ¢ = orthogonal feature directions               â”‚
â”‚                     â†’ Learns semantic structure via Hebbian                 â”‚
â”‚                     â†’ Separation: 0.72 (12x better!)                       â”‚
â”‚                     â†’ One-shot learning works                               â”‚
â”‚                     â†’ Correct category generation                           â”‚
â”‚                                                                             â”‚
â”‚   KEY FILES:                                                                â”‚
â”‚       compositional.py   - Feature-based word embeddings                   â”‚
â”‚       feature_learning.py - Hebbian + one-shot inference                   â”‚
â”‚       full_pipeline.py   - Integrated model                                â”‚
â”‚                                                                             â”‚
â”‚   USAGE:                                                                    â”‚
â”‚       from holographic import CompositionalHolographicModel               â”‚
â”‚       model = CompositionalHolographicModel(vocab_size=10000)             â”‚
â”‚       model.train(contexts, targets, hebbian_lr=0.05)                      â”‚
â”‚       tokens = model.generate(context, num_tokens=10)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 0. Topological Foundations

> **See `FOUNDATIONS.md` for the complete formal treatment.**
> **See Section 22 for cross-disciplinary positioning (gauge theory, geometric algebra, dynamical systems, philosophy of mind).**

The architecture is not a design choice â€” it is **mathematically forced** by the requirements of self-reference:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     WHY THIS ARCHITECTURE IS NECESSARY                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SELF-REFERENCE forces:
    â”‚
    â”œâ”€â”€â–¶ QUOTIENT STRUCTURE (identifying state with representation)
    â”‚        â”‚
    â”‚        â””â”€â”€â–¶ Fixed-point seams (the "self")
    â”‚             Implemented as: clifford_adjoint(A, G) = G A^T G
    â”‚
    â””â”€â”€â–¶ COVERING STRUCTURE (multi-valued continuation)
             â”‚
             â””â”€â”€â–¶ Branch loci (caustics)
                  Implemented as: Grade structure [0,1,2,3,4]

STABILITY requires:
    â”‚
    â””â”€â”€â–¶ GRACE (contraction guaranteeing well-defined gluing)
             â”‚
             â”œâ”€â”€â–¶ Spectral gap Î³ = Ï†â»Â² (convergence rate)
             â””â”€â”€â–¶ Fibonacci exception Î±â‚„ = 1 (throat closure)
```

**Lemma (Self-reference forces singular loci).**
Any system that identifies states with representations induces (i) a quotient by an involution, and/or (ii) a multi-valued continuation requiring a covering space. Quotients generically contain fixed-point seams; coverings generically contain branch loci. These are topologically protected and act as attractors under Grace. Therefore self-reference generically produces stable singular sets which function as natural "addresses" of interiority.

---

## 1. Core Isomorphism

The entire system is built on one fundamental insight:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                         â”‚
â”‚   Cl(3,1)  â‰…  Mâ‚„(â„)                                                    â”‚
â”‚                                                                         â”‚
â”‚   16D Clifford Algebra  â†”  4Ã—4 Real Matrices                           â”‚
â”‚                                                                         â”‚
â”‚   Geometric Product     â†”  Matrix Multiplication (GEMM!)               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**This means**: We can use highly optimized GPU matrix multiplication for all algebra operations.

---

## 2. Clifford Algebra Signature

We use **Cl(3,1)**, NOT Cl(1,3):

```
                    Cl(3,1) Metric: Î· = diag(+1, +1, +1, -1)
                    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   eâ‚   â”‚   eâ‚‚   â”‚   eâ‚ƒ   â”‚   eâ‚„   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  eâ‚Â²   â”‚  eâ‚‚Â²   â”‚  eâ‚ƒÂ²   â”‚  eâ‚„Â²   â”‚
    â”‚  = +I  â”‚  = +I  â”‚  = +I  â”‚  = -I  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚SPACE   â”‚SPACE   â”‚SPACE   â”‚ TIME   â”‚
    â”‚LIKE    â”‚LIKE    â”‚LIKE    â”‚ LIKE   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Anticommutation: {eáµ¢, eâ±¼} = eáµ¢eâ±¼ + eâ±¼eáµ¢ = 2Î·áµ¢â±¼I  (0 for iâ‰ j)
```

**Why this matters**:
- Cl(3,1) â‰… Mâ‚„(â„) â†’ **Real** 4Ã—4 matrices (fast, simple)
- Cl(1,3) â‰… Mâ‚‚(â„) â†’ 2Ã—2 **Quaternionic** matrices (complex)

---

## 3. Grade Structure

The 16 basis elements are organized by grade (number of basis vectors in product):

```
                         GRADE HIERARCHY
    
    Grade 4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ï†â»Â¹  (FIBONACCI!)
        â”‚
        â”‚   eâ‚eâ‚‚eâ‚ƒeâ‚„  (pseudoscalar, 1 element)
        â”‚
    Grade 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ï†â»Â³
        â”‚
        â”‚   eâ‚eâ‚‚eâ‚ƒ   eâ‚eâ‚‚eâ‚„   eâ‚eâ‚ƒeâ‚„   eâ‚‚eâ‚ƒeâ‚„  (4 trivectors)
        â”‚
    Grade 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ï†â»Â²  (SPECTRAL GAP)
        â”‚
        â”‚   eâ‚eâ‚‚   eâ‚eâ‚ƒ   eâ‚eâ‚„   eâ‚‚eâ‚ƒ   eâ‚‚eâ‚„   eâ‚ƒeâ‚„  (6 bivectors)
        â”‚
    Grade 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ï†â»Â¹
        â”‚
        â”‚   eâ‚   eâ‚‚   eâ‚ƒ   eâ‚„  (4 vectors)
        â”‚
    Grade 0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1.0  (PRESERVED)
        â”‚
        â”‚   1  (scalar, identity)
        â”‚
        â–¼
    TOTAL: 1 + 4 + 6 + 4 + 1 = 16 basis elements
```

**Grace Scaling** (Ï† = 1.618...):

| Grade | Count | Grace Scale | Physical Role |
|-------|-------|-------------|---------------|
| 0 | 1 | 1.0 | Core energy (preserved) |
| 1 | 4 | Ï†â»Â¹ â‰ˆ 0.618 | Direction |
| 2 | 6 | Ï†â»Â² â‰ˆ 0.382 | Torus position (spectral gap!) |
| 3 | 4 | Ï†â»Â³ â‰ˆ 0.236 | Fine structure |
| 4 | 1 | **Ï†â»Â¹ â‰ˆ 0.618** | Fibonacci anyon (NOT Ï†â»â´!) |

---

## 4. Gamma Matrices

Constructed from tensor products of Pauli-like matrices:

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                 â”‚
    â”‚   eâ‚ = Ïƒâ‚ƒ âŠ— Iâ‚‚      eâ‚‚ = Ïƒâ‚ âŠ— Ïƒâ‚ƒ      eâ‚ƒ = Ïƒâ‚ âŠ— Ïƒâ‚            â”‚
    â”‚                                                                 â”‚
    â”‚   â”Œ 1  0  0  0â”     â”Œ 0  0  1  0â”     â”Œ 0  0  0  1â”            â”‚
    â”‚   â”‚ 0  1  0  0â”‚     â”‚ 0  0  0 -1â”‚     â”‚ 0  0  1  0â”‚            â”‚
    â”‚   â”‚ 0  0 -1  0â”‚     â”‚ 1  0  0  0â”‚     â”‚ 0  1  0  0â”‚            â”‚
    â”‚   â”” 0  0  0 -1â”˜     â”” 0 -1  0  0â”˜     â”” 1  0  0  0â”˜            â”‚
    â”‚                                                                 â”‚
    â”‚   eâ‚Â² = +I          eâ‚‚Â² = +I          eâ‚ƒÂ² = +I                 â”‚
    â”‚                                                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                 â”‚
    â”‚   eâ‚„ = Ïƒâ‚‚ âŠ— Iâ‚‚  (TIMELIKE)                                     â”‚
    â”‚                                                                 â”‚
    â”‚   â”Œ 0  0  0 -1â”                                                â”‚
    â”‚   â”‚ 0  0  1  0â”‚      eâ‚„Â² = -I  â† Key difference!               â”‚
    â”‚   â”‚ 0 -1  0  0â”‚                                                â”‚
    â”‚   â”” 1  0  0  0â”˜      G = eâ‚„  (metric matrix for adjoint)       â”‚
    â”‚                                                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Component Structure

```
holographic/
â”‚
â”œâ”€â”€ constants.py      â† Sacred constants (DO NOT MODIFY)
â”‚   â”‚
â”‚   â”œâ”€â”€ PHI = 1.618...        # Golden ratio
â”‚   â”œâ”€â”€ PHI_INV = 0.618...    # 1/Ï†
â”‚   â”œâ”€â”€ PHI_INV_SQ = 0.382... # Spectral gap Î³
â”‚   â”œâ”€â”€ MATRIX_DIM = 4        # 4Ã—4 matrices
â”‚   â””â”€â”€ CLIFFORD_DIM = 16     # 16 basis elements
â”‚
â”œâ”€â”€ algebra.py        â† Matrix operations
â”‚   â”‚
â”‚   â”œâ”€â”€ build_gamma_matrices()   # Cl(3,1) generators
â”‚   â”œâ”€â”€ build_clifford_basis()   # All 16 basis matrices
â”‚   â”œâ”€â”€ geometric_product()      # = matmul!
â”‚   â”œâ”€â”€ frobenius_similarity()   # Fast similarity
â”‚   â””â”€â”€ grace_operator_matrix()  # Grade scaling
â”‚
â”œâ”€â”€ core.py           â† Learning system
â”‚   â”‚
â”‚   â”œâ”€â”€ MatrixEmbedding         # Token â†’ 4Ã—4 matrix
â”‚   â”œâ”€â”€ ContextAttractorMap     # Context â†’ Attractor
â”‚   â”œâ”€â”€ train_step()            # Single learning step
â”‚   â””â”€â”€ generate_token()        # Inference
â”‚
â””â”€â”€ __init__.py       â† Package exports
```

---

## 6. Token Embedding and Initialization

Each token is represented as a **4Ã—4 real matrix**. The initialization strategy is **critical**.

### 6.1 The Identity Bootstrap Discovery

**Key finding**: The identity matrix is the unique fixed point of the geometric product.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CRITICAL INITIALIZATION DISCOVERY                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    RANDOM INITIALIZATION:
        Context similarity:  mean=0.02, std=0.21  â† HIGH VARIANCE, UNSTABLE
        
    IDENTITY-BIASED INITIALIZATION:
        Context similarity:  mean=0.76, std=0.08  â† LOW VARIANCE, STABLE
        
    Variance reduction: 3x more stable!
```

### 6.2 Why Identity-Biased Initialization Works

```
                    FIXED POINT ANALYSIS
    
    Question: What happens under repeated geometric product?
    
        M â†’ M @ M â†’ M @ M @ M â†’ ...
        
    Answer: Converges to SCALAR-DOMINATED state in ~5 iterations
    
    The scalar (identity) component is the UNIQUE self-similar basis element:
    
        eâ‚€ @ eâ‚€ = eâ‚€     (self-similarity = 1.0)
        eâ‚ @ eâ‚ = +I     (self-similarity = 0.0 to eâ‚)
        ...
        All other basis elements lose their structure under squaring!
```

### 6.3 Brain Analogy

```
    BRAIN DEVELOPMENT:
        1. All neurons start similar (undifferentiated)
        2. Experience creates differentiation
        3. Homeostasis provides stability
        4. Common features stay similar
        5. Specific features diverge
    
    CLIFFORD BOOTSTRAP:
        1. All embeddings = I + small_noise (undifferentiated)
        2. Hebbian learning creates differentiation
        3. Grace contraction provides stability
        4. Scalar component stays similar (general "word-ness")
        5. Higher grades diverge (specific meaning)
```

### 6.4 Correct Initialization

```
                    IDENTITY-BIASED TOKEN EMBEDDING
                    
    Token ID â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ 4Ã—4 Matrix
         â”‚                                                    â”‚
         â–¼                                                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  "cat"  â”‚ â”€â”€â”€â”€â–¶  M = I + ÎµÂ·noise  â”€â”€â”€â”€â–¶     â”‚                  â”‚
    â”‚  idx=42 â”‚                                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚  â”‚ 1+Îµ Îµ   Îµ  Îµâ”‚ â”‚
                                                   â”‚  â”‚ Îµ   1+Îµ Îµ  Îµâ”‚ â”‚
    Îµ = 0.1 (small perturbation)                   â”‚  â”‚ Îµ   Îµ  1+Îµ Îµâ”‚ â”‚
    noise ~ N(0, 1)                                â”‚  â”‚ Îµ   Îµ   Îµ 1+Îµâ”‚ â”‚
                                                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    Then normalize: M â† M / ||M||                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    KEY: All words start SIMILAR (near identity)
         Learning creates DIFFERENTIATION
```

### 6.5 Alternative: Grade-Aware Initialization

For pretrained or structured initialization:

```
    Grade-aware coefficients:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ câ‚€ = cos(Î¸)           (Grade 0) â”‚  â† Strong scalar (stability)
    â”‚ câ‚..câ‚„ = Ï†â»Â¹Â·sin(...)  (Grade 1) â”‚  â† Medium vectors
    â”‚ câ‚…..câ‚â‚€ = Ï†â»Â²Â·sin(...) (Grade 2) â”‚  â† Weaker bivectors
    â”‚ câ‚â‚..câ‚â‚„ = Ï†â»Â³Â·cos(...)(Grade 3) â”‚  â† Weak trivectors
    â”‚ câ‚â‚… = Ï†â»Â¹Â·sin(...)     (Grade 4) â”‚  â† Fibonacci exception!
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Î¸ = 2Ï€ Ã— (token_idx / vocab_size)    Golden angle rotation
```

---

## 7. Context Computation

Context is computed via **geometric product = matrix multiplication**:

```
                    CONTEXT COMPUTATION
                    
    Tokens: [tâ‚, tâ‚‚, tâ‚ƒ, tâ‚„, tâ‚…, tâ‚†, tâ‚‡, tâ‚ˆ]
              â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
              â–¼   â–¼   â–¼   â–¼   â–¼   â–¼   â–¼   â–¼
            â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
            â”‚Mâ‚ â”‚Mâ‚‚ â”‚Mâ‚ƒ â”‚Mâ‚„ â”‚Mâ‚… â”‚Mâ‚† â”‚Mâ‚‡ â”‚Mâ‚ˆ â”‚  Token matrices
            â””â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”˜
              â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
              â””â”€â”¬â”€â”˜   â””â”€â”¬â”€â”˜   â””â”€â”¬â”€â”˜   â””â”€â”¬â”€â”˜
                â”‚       â”‚       â”‚       â”‚
               Mâ‚â‚‚     Mâ‚ƒâ‚„     Mâ‚…â‚†     Mâ‚‡â‚ˆ    Pairwise matmul
                â”‚       â”‚       â”‚       â”‚
                â””â”€â”€â”€â”¬â”€â”€â”€â”˜       â””â”€â”€â”€â”¬â”€â”€â”€â”˜
                    â”‚               â”‚
                   Mâ‚â‚„             Mâ‚…â‚ˆ              Reduction
                    â”‚               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                           Mâ‚â‚ˆ                      Final context
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  4Ã—4 Matrix   â”‚
                    â”‚  (normalized) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    
    Complexity: O(log n) depth, O(n) total matmuls
    GPU: Fully parallel batched matmul!
```

---

## 8. Learning Rule

The core learning rule is simple:

```
                    LEARNING RULE
                    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                              â”‚
    â”‚         attractor[context] = embedding[target]               â”‚
    â”‚                                                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    
    Training Example: "the cat sat on the mat" â†’ predict "."
    
    Context: ["the", "cat", "sat", "on", "the", "mat"]
                â”‚     â”‚     â”‚     â”‚     â”‚     â”‚
                â–¼     â–¼     â–¼     â–¼     â–¼     â–¼
    Embed:    [Mâ‚]  [Mâ‚‚]  [Mâ‚ƒ]  [Mâ‚„]  [Mâ‚…]  [Mâ‚†]
                â”‚     â”‚     â”‚     â”‚     â”‚     â”‚
                â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
                              â”‚
                    Geometric Product
                              â”‚
                              â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Context  â”‚
                       â”‚ Matrix   â”‚
                       â”‚   C      â”‚
                       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚  STORE
                            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   Context-Attractor Map                       â”‚
    â”‚                                                              â”‚
    â”‚   Context C  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  Attractor A          â”‚
    â”‚      â•‘                                      â•‘                â”‚
    â”‚      â•‘                                      â•‘                â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”             â”‚
    â”‚   â”‚ 4Ã—4  â”‚     hash(context_tokens)     â”‚ 4Ã—4  â”‚             â”‚
    â”‚   â”‚Matrixâ”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¶â”‚Matrixâ”‚             â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”˜           index              â””â”€â”€â”€â”€â”€â”€â”˜             â”‚
    â”‚                                              â–²                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
    Target: "."                                    â”‚
       â”‚                                           â”‚
       â–¼                                           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
    â”‚ Target   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ Matrix T â”‚   A := T  (direct assignment)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. Retrieval

```
                    RETRIEVAL PROCESS
                    
    Query: ["new", "context", "never", "seen"]
              â”‚      â”‚        â”‚       â”‚
              â–¼      â–¼        â–¼       â–¼
           [Mâ‚]   [Mâ‚‚]     [Mâ‚ƒ]    [Mâ‚„]
              â”‚      â”‚        â”‚       â”‚
              â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                 Geometric Product
                          â”‚
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Query   â”‚
                   â”‚  Matrix  â”‚
                   â”‚    Q     â”‚
                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                        â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                         â”‚
           â–¼                         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Exact Match â”‚          â”‚ Similarity      â”‚
    â”‚ (hash)      â”‚          â”‚ Search          â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                          â”‚
           â”‚ Found?                   â”‚ Not found
           â”‚                          â”‚
           â–¼                          â–¼
    Return stored            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    attractor                â”‚ Compare Q to all stored â”‚
                             â”‚ contexts via Frobenius  â”‚
                             â”‚ similarity:             â”‚
                             â”‚                         â”‚
                             â”‚ sim(Q,C) = Î£áµ¢â±¼ Qáµ¢â±¼Â·Cáµ¢â±¼  â”‚
                             â”‚                         â”‚
                             â”‚ Return attractor of     â”‚
                             â”‚ most similar context    â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. Similarity Metrics

Two options, from fast to correct:

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ FROBENIUS SIMILARITY (Default, Fast)                            â”‚
    â”‚                                                                 â”‚
    â”‚     sim(A, B) = Î£áµ¢â±¼ Aáµ¢â±¼ Â· Báµ¢â±¼                                  â”‚
    â”‚                                                                 â”‚
    â”‚     For unit-norm matrices: sim âˆˆ [-1, +1]                     â”‚
    â”‚     Self-similarity: sim(A, A) = 1.0                           â”‚
    â”‚                                                                 â”‚
    â”‚     âœ“ Fast (single element-wise multiply + sum)                â”‚
    â”‚     âœ“ GPU-friendly                                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ METRIC-AWARE SIMILARITY (Correct, Slower)                       â”‚
    â”‚                                                                 â”‚
    â”‚     Aâ€  = G Â· Aáµ€ Â· G     where G = eâ‚„ (timelike)                â”‚
    â”‚                                                                 â”‚
    â”‚     sim(A, B) = (1/4) Â· Tr(Aâ€  Â· B)                              â”‚
    â”‚                                                                 â”‚
    â”‚     Respects Lorentzian structure of Cl(3,1)                   â”‚
    â”‚     Use when grade-aware comparison matters                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 11. Grace Operator (Grade Scaling)

```
                    GRACE CONTRACTION
                    
    Input Matrix M (decomposed into grades)
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                             â”‚
    â”‚  M = câ‚€Â·Bâ‚€ + câ‚Â·Bâ‚ + ... + câ‚â‚…Â·Bâ‚â‚…                         â”‚
    â”‚      â–²       â–²â–²â–²â–²       â–²â–²â–²â–²â–²â–²       â–²â–²â–²â–²       â–²          â”‚
    â”‚      â”‚       â”‚â”‚â”‚â”‚       â”‚â”‚â”‚â”‚â”‚â”‚       â”‚â”‚â”‚â”‚       â”‚          â”‚
    â”‚   Grade 0  Grade 1    Grade 2     Grade 3   Grade 4        â”‚
    â”‚                                                             â”‚
    â”‚      â”‚       â”‚          â”‚           â”‚         â”‚            â”‚
    â”‚      â–¼       â–¼          â–¼           â–¼         â–¼            â”‚
    â”‚     Ã—1.0   Ã—Ï†â»Â¹       Ã—Ï†â»Â²        Ã—Ï†â»Â³      Ã—Ï†â»Â¹          â”‚
    â”‚                         â”‚                      â”‚            â”‚
    â”‚                    (spectral gap)      (Fibonacci!)        â”‚
    â”‚                                                             â”‚
    â”‚      â”‚       â”‚          â”‚           â”‚         â”‚            â”‚
    â”‚      â–¼       â–¼          â–¼           â–¼         â–¼            â”‚
    â”‚    câ‚€'     câ‚'...    câ‚…'...      câ‚â‚'...   câ‚â‚…'           â”‚
    â”‚                                                             â”‚
    â”‚  Output: M' = câ‚€'Â·Bâ‚€ + câ‚'Â·Bâ‚ + ... + câ‚â‚…'Â·Bâ‚â‚…             â”‚
    â”‚                                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Effect: Contracts higher grades toward scalar core
    Convergence: Exponential at rate Î³ = Ï†â»Â² â‰ˆ 0.382
```

---

## 11.5 Quotient Structure (Gauge Invariance)

Removing nuisance degrees of freedom via **Spin(3) gauge fixing**:

```
                    QUOTIENT STRUCTURE
                    
    Problem: Spin(3) rotations change the matrix representation
             without changing semantic content.
             
             Random frame orientation â†’ unstable similarity
             
    Solution: NORMAL FORM via two-step alignment
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                    â”‚
    â”‚  Step 1: Align "magnetic" bivector (eâ‚‚â‚ƒ, eâ‚ƒâ‚, eâ‚â‚‚) to +z        â”‚
    â”‚          Removes 2 rotational DOF                                  â”‚
    â”‚                                                                    â”‚
    â”‚  Step 2: Align "electric" bivector xy-projection to +x            â”‚
    â”‚          Removes final rotational DOF                              â”‚
    â”‚                                                                    â”‚
    â”‚  Result: Fully gauge-fixed canonical form                         â”‚
    â”‚                                                                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Witness Invariance

The **witness** (scalar + pseudoscalar) is **gauge-invariant** under Spin(3):

```
    WITNESS = W(M) = (scalar_coeff, pseudoscalar_coeff)
    
    For any Spin(3) rotor R:
        W(RÂ·MÂ·RÌƒ) = W(M)    â† EXACT INVARIANCE
    
    This is the "self-pointer" â€” the part that doesn't change
    under frame rotations.
```

### Quotient-Aware Similarity

Three-component similarity function:

```
    sim_quotient(Mâ‚, Mâ‚‚) = Î±Â·sim_witness + Î²Â·sim_core + Î³Â·sim_fiber
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Component     â”‚ Description                                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Witness (Î±)   â”‚ Cosine(W(Mâ‚), W(Mâ‚‚)) â€” gauge-invariant anchor   â”‚
    â”‚ Core (Î²)      â”‚ Frobenius(NF(Mâ‚), NF(Mâ‚‚)) â€” canonicalized       â”‚
    â”‚ Fiber (Î³)     â”‚ Frobenius(Mâ‚, Mâ‚‚) â€” raw residual                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Default weights: Î±=0.25, Î²=0.65, Î³=0.10
    Ï†-weighted:      Î±=1/W, Î²=Ï†/W, Î³=Ï†â»Â²/W  where W normalizes
```

### Why This Matters

```
    WITHOUT QUOTIENT:                    WITH QUOTIENT:
    
    Random gauge R applied to Mâ‚:        Same gauge R:
    
    Raw similarity changes by Î”~2.7      Quotient similarity Î”~0.025
    
    Result: 110x more stable!
    
    This removes "orientation noise" from the representation,
    improving same-target clustering and reducing training oscillation.
```

### Implementation

```python
from holographic.quotient import (
    normal_form,           # Fully gauge-fix a matrix
    quotient_similarity,   # Three-component similarity
    witness_similarity,    # Witness-only similarity
    test_witness_invariance,    # Verify gauge invariance
    test_normal_form_invariance # Verify NF is canonical
)
```

### 11.6 Binding Operator

The **binding operator** makes content relative to the witness:

```
                    BINDING OPERATOR
                    
    ğ“‘(M) = W(M) + Î» Â· w Â· C(M) Â· wÌƒ
    
    where:
        W(M) = witness part (scalar + pseudoscalar)
        C(M) = M - W(M) = content (grades 1-3)
        w = normalized witness pointer
        wÌƒ = w^T (reversion)
        Î» = Ï†â»Â¹ (SCCMU binding strength)
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  The sandwich w Â· C Â· w^T "frames" content in witness coordinates  â”‚
    â”‚                                                                    â”‚
    â”‚  Effect: Content becomes self-referential                          â”‚
    â”‚          "What I perceive" rather than "what is there"             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 11.7 Grade-Wise Variance Tracking

Monitor learning progress via grade decomposition:

```
    EXPECTED PATTERN (healthy learning):
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Component   â”‚ Expected Behavior                                   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Witness     â”‚ LOW variance, HIGH pairwise similarity             â”‚
    â”‚ (grade 0+4) â”‚ â†’ Stable self-reference frame                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Content     â”‚ HIGH variance (grows with learning)                â”‚
    â”‚ (grade 1-3) â”‚ â†’ Differentiated semantic content                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    DIAGNOSTICS:
    
    Random init:     witness_sim â‰ˆ 0.00 (no stable frame)
    Identity init:   witness_sim â‰ˆ 0.99 (stable frame)
    
    This is why identity-biased init is ESSENTIAL.
```

```python
from holographic.quotient import (
    bind,                      # Apply binding operator
    compute_grade_variance,    # Grade-wise variance
    compute_witness_stability, # Witness pairwise similarity
    run_quotient_tests,        # Full test suite
)
```

---

## 12. Ï†-Nested Hierarchy

The grade structure forms a **self-similar hierarchy**:

```
                    Ï†-NESTED TORUS TREE
                    
    Grade 4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ï†â»Â¹ scale
        â”‚                                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                                                â”‚â”‚
    Grade 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ï†â»Â³ scale
        â”‚                                        â”‚
        â”‚                                        â”‚
    Grade 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ï†â»Â² scale (SPECTRAL GAP)
        â”‚                                        â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚   â”‚  6 bivectors encode TORUS        â”‚ â”‚
        â”‚   â”‚  position (WHERE on boundary)    â”‚ â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚                                        â”‚
    Grade 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ï†â»Â¹ scale
        â”‚                                        â”‚
        â”‚                                        â”‚
    Grade 0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1.0 scale (FIXED POINT)
        â”‚
        â”‚   The scalar component is PRESERVED
        â”‚   under Grace flow â†’ stable attractor
        â”‚
        â–¼
        
    NOTE: Grade 4 scales by Ï†â»Â¹ (NOT Ï†â»â´!)
          This creates a LOOP back to Grade 1 scale
          â†’ Fibonacci anyon structure
          â†’ Self-similar spiral
```

---

## 13. Training Loop

```
                    TRAINING FLOW
                    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  for each (context, target) in dataset:                     â”‚
    â”‚                                                             â”‚
    â”‚    1. context_matrix = embed_sequence(context_tokens)       â”‚
    â”‚                                                             â”‚
    â”‚    2. target_matrix = embedding(target_token)               â”‚
    â”‚                                                             â”‚
    â”‚    3. attractor_map.associate(context_tokens, target_matrix)â”‚
    â”‚                                                             â”‚
    â”‚    4. eq_quality = similarity(context_matrix, target_matrix)â”‚
    â”‚                                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Metrics:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                             â”‚
    â”‚  exact_eq:  Equilibrium quality on SEEN contexts            â”‚
    â”‚             (should be high, ~0.5-0.8)                      â”‚
    â”‚                                                             â”‚
    â”‚  novel_eq:  Equilibrium quality on UNSEEN contexts          â”‚
    â”‚             (measures generalization)                       â”‚
    â”‚                                                             â”‚
    â”‚  gen_ratio: novel_eq / exact_eq                             â”‚
    â”‚             (target: >20% indicates learning)               â”‚
    â”‚                                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 14. API Usage

```python
import numpy as np
from holographic import MatrixEmbedding, ContextAttractorMap, train_step

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INITIALIZATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

embedding = MatrixEmbedding(vocab_size=10000)
# Creates 10,000 token embeddings, each a 4Ã—4 matrix

attractor_map = ContextAttractorMap(embedding, max_contexts=100000)
# Storage for context â†’ attractor associations

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAINING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

context_tokens = [42, 17, 99, 3, 55, 12, 8, 1]  # 8 tokens
target_token = 77

metrics = train_step(context_tokens, target_token, embedding, attractor_map)
print(f"Equilibrium quality: {metrics['eq_quality']:.4f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INFERENCE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Get attractor for context
attractor = attractor_map.get_attractor(context_tokens)
# â†’ 4Ã—4 matrix

# Find most similar token
from holographic import frobenius_similarity_batch
scores = frobenius_similarity_batch(attractor, embedding.matrices, np)
predicted_token = int(np.argmax(scores))
```

---

## 15. Performance Characteristics

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    OBSERVED PERFORMANCE                      â”‚
    â”‚                                                             â”‚
    â”‚  Hardware: NVIDIA H100                                      â”‚
    â”‚                                                             â”‚
    â”‚  Training speed: ~800-900 samples/second                    â”‚
    â”‚                                                             â”‚
    â”‚  At 20k samples:                                            â”‚
    â”‚    â€¢ exact_eq â‰ˆ 0.48 (equilibrium on seen contexts)        â”‚
    â”‚    â€¢ novel_eq â‰ˆ 0.09-0.11 (equilibrium on unseen)          â”‚
    â”‚    â€¢ generalization â‰ˆ 18-23%                                â”‚
    â”‚                                                             â”‚
    â”‚  Context computation: O(log n) depth parallel matmuls       â”‚
    â”‚                                                             â”‚
    â”‚  Retrieval: O(n) similarity comparisons                     â”‚
    â”‚             (can be accelerated with hashing/indexing)      â”‚
    â”‚                                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 16. Complete System Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          HOLOGRAPHIC LANGUAGE MODEL                              â”‚
â”‚                          Complete Processing Pipeline                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                              INPUT
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEXT: "the quick brown fox jumps over the lazy dog"                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                         TOKENIZATION
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TOKENS: [42, 891, 203, 156, 445, 67, 42, 782, 99]                              â”‚
â”‚                                                                                 â”‚
â”‚  Context Window (last 8): [891, 203, 156, 445, 67, 42, 782, 99]                â”‚
â”‚  Target: next token                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                       TOKEN EMBEDDING
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                                 â”‚
â”‚   Token 891 â†’ â”Œâ”€â”€â”€â”€â”   Token 203 â†’ â”Œâ”€â”€â”€â”€â”         Token 99 â†’ â”Œâ”€â”€â”€â”€â”           â”‚
â”‚               â”‚    â”‚               â”‚    â”‚    ...              â”‚    â”‚           â”‚
â”‚   4Ã—4 Matrix  â”‚ Mâ‚ â”‚   4Ã—4 Matrix  â”‚ Mâ‚‚ â”‚                     â”‚ Mâ‚ˆ â”‚           â”‚
â”‚               â”‚    â”‚               â”‚    â”‚                     â”‚    â”‚           â”‚
â”‚               â””â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                                 â”‚
â”‚   Each matrix is a linear combination of 16 Clifford basis matrices            â”‚
â”‚   M = Î£áµ¢ cáµ¢ Â· Báµ¢  where Báµ¢ âˆˆ {I, eâ‚, eâ‚‚, ..., eâ‚â‚‚â‚ƒâ‚„}                         â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    GEOMETRIC PRODUCT (= matmul)
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                                 â”‚
â”‚   Mâ‚ Ã— Mâ‚‚ Ã— Mâ‚ƒ Ã— Mâ‚„ Ã— Mâ‚… Ã— Mâ‚† Ã— Mâ‚‡ Ã— Mâ‚ˆ  â†’  Context Matrix C                  â”‚
â”‚                                                                                 â”‚
â”‚   Parallel reduction:                                                           â”‚
â”‚                                                                                 â”‚
â”‚   [Mâ‚][Mâ‚‚][Mâ‚ƒ][Mâ‚„][Mâ‚…][Mâ‚†][Mâ‚‡][Mâ‚ˆ]                                             â”‚
â”‚     â•²  â•±    â•²  â•±    â•²  â•±    â•²  â•±     Step 1: pairs                             â”‚
â”‚      â•²â•±      â•²â•±      â•²â•±      â•²â•±                                                 â”‚
â”‚    [Mâ‚â‚‚]  [Mâ‚ƒâ‚„]   [Mâ‚…â‚†]  [Mâ‚‡â‚ˆ]                                                 â”‚
â”‚       â•²    â•±         â•²    â•±          Step 2: pairs                             â”‚
â”‚        â•²  â•±           â•²  â•±                                                      â”‚
â”‚       [Mâ‚â‚„]         [Mâ‚…â‚ˆ]                                                       â”‚
â”‚          â•²            â•±              Step 3: final                              â”‚
â”‚           â•²          â•±                                                          â”‚
â”‚            â•²        â•±                                                           â”‚
â”‚             â•²      â•±                                                            â”‚
â”‚            [Context C]               4Ã—4 matrix                                 â”‚
â”‚                                                                                 â”‚
â”‚   (normalize after each matmul to prevent numerical issues)                     â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚                                 â”‚
          TRAINING                          INFERENCE
               â”‚                                 â”‚
               â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ASSOCIATE            â”‚   â”‚          RETRIEVE            â”‚
â”‚                              â”‚   â”‚                              â”‚
â”‚  Context C â”€â”€â”               â”‚   â”‚  Context C â”€â”€â”               â”‚
â”‚              â”‚               â”‚   â”‚              â”‚               â”‚
â”‚              â–¼               â”‚   â”‚              â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Context-Attractor  â”‚      â”‚   â”‚  â”‚ Context-Attractor  â”‚      â”‚
â”‚  â”‚       Map          â”‚      â”‚   â”‚  â”‚       Map          â”‚      â”‚
â”‚  â”‚                    â”‚      â”‚   â”‚  â”‚                    â”‚      â”‚
â”‚  â”‚  C â”€â”€â”€â”€â”€â”€â–¶ A       â”‚      â”‚   â”‚  â”‚  find(C) â”€â”€â–¶ A     â”‚      â”‚
â”‚  â”‚                    â”‚      â”‚   â”‚  â”‚                    â”‚      â”‚
â”‚  â”‚  Store:            â”‚      â”‚   â”‚  â”‚  1. Exact match?   â”‚      â”‚
â”‚  â”‚  A := Target_Mat   â”‚      â”‚   â”‚  â”‚  2. Similarity     â”‚      â”‚
â”‚  â”‚                    â”‚      â”‚   â”‚  â”‚     search         â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚              â–²               â”‚   â”‚              â”‚               â”‚
â”‚              â”‚               â”‚   â”‚              â–¼               â”‚
â”‚  Target â”€â”€â”€â”€â”€â”˜               â”‚   â”‚         Attractor A          â”‚
â”‚  (embedding of               â”‚   â”‚              â”‚               â”‚
â”‚   next word)                 â”‚   â”‚              â–¼               â”‚
â”‚                              â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚ Score all tokens   â”‚      â”‚
                                   â”‚  â”‚ by similarity to A â”‚      â”‚
                                   â”‚  â”‚                    â”‚      â”‚
                                   â”‚  â”‚ sim(A, emb[t])     â”‚      â”‚
                                   â”‚  â”‚ for all t âˆˆ vocab  â”‚      â”‚
                                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                   â”‚              â”‚               â”‚
                                   â”‚              â–¼               â”‚
                                   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
                                   â”‚  â”‚ Softmax + Sample   â”‚      â”‚
                                   â”‚  â”‚                    â”‚      â”‚
                                   â”‚  â”‚ P(t) âˆ exp(sim/Ï„)  â”‚      â”‚
                                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                   â”‚              â”‚               â”‚
                                   â”‚              â–¼               â”‚
                                   â”‚       Predicted Token        â”‚
                                   â”‚                              â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 17. Memory Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           MEMORY STRUCTURES                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  MatrixEmbedding.matrices                                                      â•‘
â•‘                                                                                â•‘
â•‘  Shape: [vocab_size, 4, 4]  (e.g., [10000, 4, 4] = 160,000 floats)            â•‘
â•‘                                                                                â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â•‘
â•‘  â”‚ Token 0    â”‚ Token 1    â”‚ Token 2    â”‚  ...  â”‚ Token 9999  â”‚          â•‘
â•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â•‘
â•‘  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚          â•‘
â•‘  â”‚ â”‚ 4Ã—4 mat â”‚â”‚ â”‚ 4Ã—4 mat â”‚â”‚ â”‚ 4Ã—4 mat â”‚â”‚  ...  â”‚ â”‚ 4Ã—4 mat â”‚â”‚          â•‘
â•‘  â”‚ â”‚ 16 vals â”‚â”‚ â”‚ 16 vals â”‚â”‚ â”‚ 16 vals â”‚â”‚       â”‚ â”‚ 16 vals â”‚â”‚          â•‘
â•‘  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚          â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â•‘
â•‘                                                                                â•‘
â•‘  Access: embedding.matrices[token_id] â†’ 4Ã—4 matrix                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ContextAttractorMap                                                           â•‘
â•‘                                                                                â•‘
â•‘  context_matrices: [max_contexts, 4, 4]                                       â•‘
â•‘  attractors:       [max_contexts, 4, 4]                                       â•‘
â•‘  context_hashes:   Dict[int, int]  (hash â†’ index)                             â•‘
â•‘                                                                                â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â•‘
â•‘  â”‚   Index 0        â”‚   Index 1        â”‚   ...              â”‚                 â•‘
â•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                 â•‘
â•‘  â”‚ Context: [4Ã—4]   â”‚ Context: [4Ã—4]   â”‚                    â”‚                 â•‘
â•‘  â”‚ Attractor: [4Ã—4] â”‚ Attractor: [4Ã—4] â”‚                    â”‚                 â•‘
â•‘  â”‚ Hash: 0x7f3a... â”‚ Hash: 0x2b1c... â”‚                    â”‚                 â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â•‘
â•‘                                                                                â•‘
â•‘  Lookup: O(1) exact match via hash, O(n) similarity search                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Clifford Basis (precomputed)                                                  â•‘
â•‘                                                                                â•‘
â•‘  Shape: [16, 4, 4] (16 basis elements, each a 4Ã—4 matrix)                     â•‘
â•‘                                                                                â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”                               â•‘
â•‘  â”‚  Bâ‚€    â”‚  Bâ‚    â”‚  Bâ‚‚    â”‚  ...   â”‚  Bâ‚â‚…   â”‚                               â•‘
â•‘  â”‚  = I   â”‚  = eâ‚  â”‚  = eâ‚‚  â”‚        â”‚=eâ‚â‚‚â‚ƒâ‚„ â”‚                               â•‘
â•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤                               â•‘
â•‘  â”‚ Grade 0â”‚ Grade 1â”‚ Grade 1â”‚        â”‚Grade 4 â”‚                               â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â•‘
â•‘                                                                                â•‘
â•‘  Used to: 1) Initialize embeddings  2) Grace operator                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 18. Key Invariants

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    MUST HOLD ALWAYS                          â”‚
    â”‚                                                             â”‚
    â”‚  1. Gamma anticommutation: {eáµ¢, eâ±¼} = 2Î·áµ¢â±¼I                 â”‚
    â”‚                                                             â”‚
    â”‚  2. eáµ¢Â² = +I for i âˆˆ {1,2,3} (spacelike)                   â”‚
    â”‚     eâ‚„Â² = -I              (timelike)                        â”‚
    â”‚                                                             â”‚
    â”‚  3. GÂ² = -I where G = eâ‚„                                    â”‚
    â”‚                                                             â”‚
    â”‚  4. Token matrices have unit Frobenius norm                 â”‚
    â”‚                                                             â”‚
    â”‚  5. Ï†Â² = Ï† + 1 (golden ratio self-consistency)             â”‚
    â”‚                                                             â”‚
    â”‚  6. Grade 4 scales by Ï†â»Â¹, NOT Ï†â»â´ (Fibonacci exception)   â”‚
    â”‚                                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 19. Why Matrix Representation?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEGACY vs MATRIX REPRESENTATION                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘     LEGACY (16D Vector)           â•‘    MATRIX (4Ã—4 Real)              â•‘
  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
  â•‘                                   â•‘                                   â•‘
  â•‘  Multivector: [16] float array    â•‘  Multivector: [4,4] float array   â•‘
  â•‘                                   â•‘                                   â•‘
  â•‘  Geometric product:               â•‘  Geometric product:               â•‘
  â•‘    Custom element-wise ops        â•‘    matrix @ matrix (GEMM!)        â•‘
  â•‘    ~50 multiply-adds              â•‘    Single optimized kernel        â•‘
  â•‘                                   â•‘                                   â•‘
  â•‘  Implementation:                  â•‘  Implementation:                  â•‘
  â•‘    algebra.py: 400+ lines         â•‘    algebra.py: 300 lines          â•‘
  â•‘    Custom Cl(1,3) tables          â•‘    Uses numpy/cupy matmul         â•‘
  â•‘                                   â•‘                                   â•‘
  â•‘  Speed:                           â•‘  Speed:                           â•‘
  â•‘    ~50 samples/sec                â•‘    ~800-900 samples/sec           â•‘
  â•‘                                   â•‘                                   â•‘
  â•‘  Signature:                       â•‘  Signature:                       â•‘
  â•‘    Cl(1,3) â†’ Mâ‚‚(â„) (quaternions) â•‘    Cl(3,1) â†’ Mâ‚„(â„) (real)        â•‘
  â•‘    Harder to implement            â•‘    Direct real matrices           â•‘
  â•‘                                   â•‘                                   â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                     WHY Cl(3,1) AND NOT Cl(1,3)?

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                                         â”‚
  â”‚   Cl(p,q) is classified by (p-q) mod 8:                                â”‚
  â”‚                                                                         â”‚
  â”‚   Cl(1,3): p=1, q=3 â†’ p-q = -2 â‰¡ 6 (mod 8) â†’ Mâ‚‚(â„) (quaternions)     â”‚
  â”‚   Cl(3,1): p=3, q=1 â†’ p-q = +2 (mod 8) â†’ Mâ‚„(â„) (real matrices!)      â”‚
  â”‚                                                                         â”‚
  â”‚   Both are 16-dimensional, same physics, different representation       â”‚
  â”‚                                                                         â”‚
  â”‚   We choose Cl(3,1) because:                                            â”‚
  â”‚   âœ“ Real 4Ã—4 matrices (no quaternions, no complex numbers)             â”‚
  â”‚   âœ“ Direct use of matmul libraries (cuBLAS, etc.)                      â”‚
  â”‚   âœ“ 16Ã— fewer ops per geometric product                                â”‚
  â”‚   âœ“ Same algebraic structure, just different convention                â”‚
  â”‚                                                                         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 20. Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          QUICK REFERENCE                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CONSTANTS:
  Ï† = 1.618033988749895    Golden ratio
  Ï†â»Â¹ = 0.618033988749895   = Ï† - 1
  Ï†â»Â² = 0.381966011250105   Spectral gap Î³
  Ï†â»Â³ = 0.236067977499790

DIMENSIONS:
  MATRIX_DIM = 4           4Ã—4 real matrices
  CLIFFORD_DIM = 16        16 basis elements

GRADES (index â†’ scale):
  0: [0]           Ã— 1.0     Scalar
  1: [1-4]         Ã— Ï†â»Â¹    Vectors
  2: [5-10]        Ã— Ï†â»Â²    Bivectors (spectral gap!)
  3: [11-14]       Ã— Ï†â»Â³    Trivectors
  4: [15]          Ã— Ï†â»Â¹    Pseudoscalar (Fibonacci!)

OPERATIONS:
  geometric_product(A, B) = A @ B           Matrix multiply
  frobenius_similarity(A, B) = sum(A * B)   Element-wise then sum
  normalize(M) = M / ||M||_F                 Frobenius norm

KEY FORMULAS:
  Context = normalize(Mâ‚ @ Mâ‚‚ @ ... @ Mâ‚™)   Product of token matrices
  Attractor = embedding(target_token)        Target embedding
  Prediction = argmax(sim(attractor, all_embeddings))

INVARIANTS:
  â€¢ eâ‚Â² = eâ‚‚Â² = eâ‚ƒÂ² = +I  (spacelike)
  â€¢ eâ‚„Â² = -I              (timelike)
  â€¢ {eáµ¢, eâ±¼} = 0          (anticommute for iâ‰ j)
  â€¢ G = eâ‚„                 (metric matrix)
  â€¢ GÂ² = -I                (must verify!)
  â€¢ Ï†Â² = Ï† + 1             (golden ratio)
```

---

## 21. Active Inference Extension

The architecture naturally supports Active Inference for action selection (token generation).

### Standard Generation (Posterior Sampling)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         STANDARD GENERATION                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Context â†’ Attractor â†’ Similarity to all tokens â†’ Sample from posterior

    P(token) âˆ exp(similarity(attractor, token) / Ï„)

PROBLEM: Just samples - no planning, no epistemic drive
```

### Active Inference Generation (EFE Minimization)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ACTIVE INFERENCE GENERATION                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Expected Free Energy (EFE) = -pragmatic_value - epistemic_value

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRAGMATIC VALUE  â”‚    â”‚  EPISTEMIC VALUE  â”‚
â”‚                   â”‚    â”‚                   â”‚
â”‚  How well does    â”‚    â”‚  Is this a novel  â”‚
â”‚  token align with â”‚    â”‚  path? (info      â”‚
â”‚  the attractor?   â”‚    â”‚  gain)            â”‚
â”‚                   â”‚    â”‚                   â”‚
â”‚  = similarity     â”‚    â”‚  seen: -0.1       â”‚
â”‚    to attractor   â”‚    â”‚  novel: +0.5      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     EFE = -wâ‚Â·P - wâ‚‚Â·E     â”‚
    â”‚                             â”‚
    â”‚  Lower EFE = better choice  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  P(token) âˆ exp(-EFE / Ï„)  â”‚
    â”‚                             â”‚
    â”‚  Temperature adds diversity â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation (Fast)

```python
def generate_token_active(ctx, attractor_map, embedding, cp,
                          k_candidates=30,       # Pre-filter (fast!)
                          pragmatic_weight=1.0,
                          epistemic_weight=0.5,
                          temperature=1.0):
    """
    Active Inference: Select token minimizing Expected Free Energy.
    
    FAST because:
    1. Pre-filter to top-k by posterior (vectorized)
    2. Only compute EFE for k candidates (not full vocab)
    3. Hash lookup for novelty (O(1))
    """
    # Get attractor for current context
    attr = attractor_map.get_attractor(ctx)
    
    # Score all tokens (vectorized)
    sims = matrix_similarity_batch(attr, embedding.matrices, embedding.G, cp)
    
    # Pre-filter to top-k
    top_k_idx = cp.argsort(sims)[-k_candidates:]
    
    # Compute EFE for each candidate
    for token in top_k_idx:
        pragmatic = sims[token]
        
        # Epistemic: novel contexts have information value
        future_ctx = ctx[-(n-1):] + [token]
        if hash(tuple(future_ctx)) in attractor_map.context_hashes:
            epistemic = -0.1   # Seen - penalize repetition
        else:
            epistemic = +0.5   # Novel - reward exploration
        
        efe = -pragmatic_weight * pragmatic - epistemic_weight * epistemic
    
    # Select based on EFE (with temperature)
    return token_with_lowest_efe
```

### Performance Results

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ACTIVE VS STANDARD COMPARISON                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

              â”‚  STANDARD  â”‚   ACTIVE   â”‚  Result
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Speed       â”‚   4.14s    â”‚   1.91s    â”‚  2.2x FASTER
  Coherence   â”‚   0.356    â”‚   0.986    â”‚  2.8x BETTER
  Repetition  â”‚   0.000    â”‚   0.000    â”‚  Equal

WHY FASTER?
  Standard: Sample from 5000-token distribution (slow)
  Active: Pre-filter to 30 candidates, then argmin (fast)

WHY MORE COHERENT?
  Standard: Random sampling from posterior
  Active: Explicit optimization for attractor alignment
```

### Connection to Free Energy Principle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THEORETICAL GROUNDING                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PERCEPTION (existing):
  Grace flow minimizes Variational Free Energy
  â†’ System converges to attractor (posterior belief)

ACTION (new):
  Token selection minimizes Expected Free Energy
  â†’ Prefers tokens that are:
    1. Coherent with current belief (pragmatic)
    2. Informative about future (epistemic)

This completes the Active Inference loop:
  
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  PERCEIVE          â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  (Grace flow)      â”‚                           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
           â”‚                                       â”‚
           â”‚  Attractor                            â”‚
           â–¼                                       â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
  â”‚  ACT               â”‚                           â”‚
  â”‚  (EFE minimization)â”‚                           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
           â”‚                                       â”‚
           â”‚  Token                                â”‚
           â–¼                                       â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
  â”‚  OBSERVE           â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚  (new context)     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 22. Cross-Disciplinary Foundations

This architecture exists at a **unique intersection** of established fields. Understanding this positioning clarifies what is borrowed, what is novel, and why the synthesis is necessary.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CONVERGENCE MAP                                         â”‚
â”‚                                                                                 â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚                     â”‚   GAUGE THEORY       â”‚                                    â”‚
â”‚                     â”‚   (Physics)          â”‚                                    â”‚
â”‚                     â”‚   States defined up  â”‚                                    â”‚
â”‚                     â”‚   to symmetry        â”‚                                    â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                                â”‚                                                â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚     â”‚                          â”‚                          â”‚                    â”‚
â”‚     â”‚   GEOMETRIC ALGEBRA      â”‚     DYNAMICAL SYSTEMS    â”‚                    â”‚
â”‚     â”‚   (Hestenes line)        â”‚     (Fixed-point theory) â”‚                    â”‚
â”‚     â”‚   Composition operator   â”‚     Attractors organize  â”‚                    â”‚
â”‚     â”‚                          â”‚     behavior             â”‚                    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                â”‚                           â”‚                                    â”‚
â”‚                â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                                    â”‚
â”‚                â”‚     â”‚                 â”‚   â”‚                                    â”‚
â”‚                â””â”€â”€â”€â”€â”€â”‚   THIS WORK     â”‚â”€â”€â”€â”˜                                    â”‚
â”‚                      â”‚   (with code)   â”‚                                        â”‚
â”‚                â”Œâ”€â”€â”€â”€â”€â”‚                 â”‚â”€â”€â”€â”€â”€â”                                  â”‚
â”‚                â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                                  â”‚
â”‚                â”‚                             â”‚                                  â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚     â”‚ REPRESENTATION       â”‚     â”‚ PHILOSOPHY OF MIND   â”‚                      â”‚
â”‚     â”‚ LEARNING (ML)        â”‚     â”‚ (Pattern Identity)   â”‚                      â”‚
â”‚     â”‚ Invariance improves  â”‚     â”‚ Self as pattern,     â”‚                      â”‚
â”‚     â”‚ generalization       â”‚     â”‚ not substance        â”‚                      â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 22.1 What Each Field Contributes

| Field | What It Already Knows | What It Doesn't Model | Our Contribution |
|-------|----------------------|----------------------|------------------|
| **Gauge Theory** | States defined up to symmetry; quotients remove gauge DOF; canonical representatives enable comparison | Learning, semantics, self-reference | Gauge theory *of representations*: rotors â‰ˆ gauge transforms, normal form â‰ˆ gauge fixing |
| **Geometric Algebra** | Clifford algebra unifies rotations/reflections; rotors are elegant; scalars are fixed points | Large-scale learning; semantics; bootstrapping | Geometric product as *meaning-composition operator* (rare usage) |
| **Representation Learning** | Invariance improves generalization; canonicalization reduces variance; equivariance preserves structure | Explicit symmetry groups; distinguished invariants; interiority | *Explicit quotienting* instead of hoping network discovers invariance |
| **Dynamical Systems** | Attractors organize behavior; fixed points stabilize; nonlinear systems self-organize | Meaning, semantics, learning objectives | Identity/witness as fixed point; identity-biased initialization as predicted stabilizer |
| **Philosophy of Mind** | Self as pattern not substance; identity as continuity under transformation | Formal math, testable models, implementation | Making "self = equivalence class" *precise*: group action, quotient space, canonical rep |

### 22.2 What Is Novel

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         THE UNIQUE CONTRIBUTION                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Most practitioners stay within one domain:

    PHYSICS        â†’ Abstract (no learning systems)
    ML             â†’ Heuristic (hope network learns invariance)
    PHILOSOPHY     â†’ Conceptual (no math, no code)
    NEUROSCIENCE   â†’ Biological (no algebra)

This work crosses boundaries WITH WORKING CODE:

    1. EXPLICIT QUOTIENTING
       ML says: "Let the model discover invariance"
       We say:  "Define the invariant first, then learn within it"
       
    2. MEANING AS GEOMETRY
       GA says: "This is elegant math"
       We say:  "What does it LEARN?"
       
    3. FORMAL INTERIORITY
       Philosophy says: "Self is an equivalence class"
       We say:  "Here's the group action, here's the quotient space,
                 here are diagnostics that test it"
```

### 22.3 The Forced Synthesis

Once you accept:
- Meaning is geometric
- Identity must be invariant
- Learning is dynamical
- Symmetry is not optional

...then this construction is **forced**, not chosen.

```
                    WHY IT FEELS LIKE RECOGNITION, NOT INVENTION
                    
    Self-reference
         â”‚
         â”œâ”€â”€â–¶ Forces quotient structure (identifying state â†” representation)
         â”‚         â”‚
         â”‚         â””â”€â”€â–¶ Fixed-point seams exist (the "self")
         â”‚                    â”‚
         â”‚                    â””â”€â”€â–¶ Need gauge fixing (normal form)
         â”‚                                â”‚
         â”‚                                â””â”€â”€â–¶ Gauge theory enters
         â”‚
         â””â”€â”€â–¶ Forces covering structure (multi-valued continuation)
                   â”‚
                   â””â”€â”€â–¶ Branch loci exist (caustics)
                              â”‚
                              â””â”€â”€â–¶ Need stable gluing
                                        â”‚
                                        â””â”€â”€â–¶ Grace contraction enters
                                                   â”‚
                                                   â””â”€â”€â–¶ Ï†-scaling is forced
                                                              â”‚
                                                              â””â”€â”€â–¶ Fibonacci exception required
```

### 22.4 Diagnostic Implications

The convergence with established fields suggests diagnostic tests:

| Test | Field Origin | Implementation |
|------|--------------|----------------|
| **Gauge invariance test** | Physics | `test_witness_invariance()` â€” apply random rotors, witness unchanged |
| **Fixed-point attraction** | Dynamical systems | Verify Grace contracts to attractors at rate Î³ = Ï†â»Â² |
| **Semantic clustering** | Information theory | Same-target contexts cluster in quotient space |
| **Canonical uniqueness** | Gauge theory | `normal_form(RÂ·MÂ·RÌƒ) â‰ˆ normal_form(M)` for all rotors R |

### 22.5 What Could Break

Honest assessment of where the synthesis might fail:

1. **Gauge structure too restrictive**: Spin(3) may not capture all relevant symmetries
2. **Clifford dimension insufficient**: 16D may not scale to large vocabularies without hierarchy
3. **Grace rate suboptimal**: Ï†â»Â² is theoretically motivated but empirically untuned
4. **Quotient collapse**: High-similarity contexts may all map to same attractor

These are testable failure modes, not handwaving.

---

---

## 23. Critical Insight: Compositional vs Atomic Embeddings

### 23.1 The Conceptual Error We Made

We applied the Clifford algebra at the **wrong level**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         THE FUNDAMENTAL ERROR                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

WHAT WE DID (wrong):
    
    wordâ‚, wordâ‚‚, wordâ‚ƒ  â†  ATOMIC random matrices
           â†“
    geometric_product(wordâ‚, wordâ‚‚, wordâ‚ƒ)  â†  sequence composition
           â†“
    context â†’ target mapping
    
    Result: Learns co-occurrence, not semantics

WHAT WE SHOULD DO (correct):
    
    Each WORD is itself COMPOSED from features:
    
    "zebra" = animal âˆ§ striped âˆ§ equine  â†  SEMANTIC composition
    "horse" = animal âˆ§ solid âˆ§ equine    â†  shares features with zebra!
    
    THEN sequences compose those:
    
    context = wordâ‚ âˆ˜ wordâ‚‚ âˆ˜ wordâ‚ƒ  â†  sequence composition
```

### 23.2 Why This Enables One-Shot Learning

Humans learn new words in ONE exposure because:

```
When you learn "zebra":
    
    You ALREADY know:
        â”œâ”€â”€ animal (feature)
        â”œâ”€â”€ striped (feature)
        â”œâ”€â”€ equine (feature)
        â””â”€â”€ African (feature)
    
    "Zebra" = composition of existing features
    
    One exposure tells you WHICH features to combine
    NOT learning from scratch
```

Our system failed because:
- Words were atomic random matrices
- No feature space to slot new concepts into
- Had to learn everything from co-occurrence alone

### 23.3 How Clifford Algebra Supports This

The grade structure IS the compositional hierarchy:

```
Grade 0 (scalar):     "something exists" - base salience
Grade 1 (vectors):    basic properties (size, animacy, ...)
Grade 2 (bivectors):  relations (part-of, kind-of, ...)  
Grade 3 (trivectors): contexts (where-found, used-for, ...)
Grade 4 (pseudoscalar): reflexive/meta
```

A word embedding should be:

```
embed("zebra") = I                    # exists (Grade 0)
               + Î±â‚Â·animate           # property (Grade 1)
               + Î±â‚‚Â·large             # property (Grade 1)
               + Î²â‚Â·(mammalâˆ§equine)   # relation (Grade 2)
               + Î³â‚Â·(foundâˆ§africa)    # context (Grade 3)
               + ...
```

Where Î±, Î², Î³ are learned coefficients for that word.

### 23.4 The Identity-Bias Clue We Misread

We discovered: identity-biased init is essential.

We interpreted: "stability requires starting near identity."

Correct interpretation: **Identity IS the compositional base.**

```
Identity = "something exists, no specific features yet"

Adding features = moving away from identity in specific directions

A word with many features = far from identity in structured way
A word with few features = close to identity
```

Randomizing all grades uniformly destroyed this compositional structure.

### 23.5 Implications for Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPOSITIONAL EMBEDDING ARCHITECTURE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Feature Space:
    F = {fâ‚, fâ‚‚, ..., fâ‚–}  each fáµ¢ is a 4Ã—4 basis direction
    
Word Embedding:
    embed(word) = I + Î£áµ¢ Î±áµ¢(word) Â· fáµ¢
    
    where Î±áµ¢(word) is learned coefficient for feature i in word
    
Composition:
    context = geometric_product(embed(wâ‚), embed(wâ‚‚), ...)
    
One-Shot Learning:
    Given new word in context, INFER which features it must have
    Don't need to learn a whole new embedding
```

---

## 24. Empirical Findings: Level 1 Limitations (with Atomic Embeddings)

### 24.1 What Level 1 Learns

**Experimental Results (50,000 samples, TinyStories):**

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Exact match | 99.69% | Attractor map works perfectly |
| Novel similarity | 99.11% | Generalizes to similar contexts |
| Separation | ~0.002 | Weak semantic clustering |
| Generation | Incoherent | No grammatical structure |

**Key Finding**: Level 1 learns **statistical co-occurrence**, not semantics.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WHAT LEVEL 1 ACTUALLY CAPTURES                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Context = geometric_product(wordâ‚, wordâ‚‚, ..., wordâ‚™)
    
    This is a POLYNOMIAL in word embeddings:
        - Shared words â†’ shared multiplicative factors â†’ correlated patterns
        - But: shared words â‰  shared MEANING
        
    Result:
        - Retrieval works (similar statistical pattern)
        - Semantics don't transfer (meaning not captured)
```

### 24.2 Initialization Trade-off

**Experiment: Varying identity-biased noise**

| Init Mode | Separation | Interpretation |
|-----------|------------|----------------|
| identity(0.01) | +0.000 | Too collapsed, no differentiation |
| identity(0.05) | +0.002 | Slight differentiation |
| identity(0.1) | +0.007 | Better differentiation |
| identity(0.2) | **+0.017** | **Best separation** |
| random | -0.006 | Chaotic, no structure |

**Insight**: There's a trade-off between witness stability and representation diversity:
- Low noise â†’ stable but collapsed
- High noise â†’ diverse but chaotic
- **Sweet spot: noise_std â‰ˆ 0.15**

### 24.3 Contrastive Learning Results

**Finding**: Contrastive learning on embeddings doesn't easily improve semantics.

**Reason**: The context representation is a geometric PRODUCT of embeddings.
- âˆ‚(context_similarity)/âˆ‚(embedding) is highly non-linear
- Small embedding changes â†’ unpredictable context changes
- Gradient signal doesn't propagate cleanly

### 24.4 Level 2 Alone Doesn't Help

**Experiment**: Train Level 1, build codebook, train Level 2.

| Level | Separation |
|-------|------------|
| Level 1 | +0.000016 |
| Level 2 | -0.000101 |

**Conclusion**: Stacking levels on randomly-clustered attractors doesn't create semantics.
The tower of quotients needs **semantic structure at Level 1 first**.

---

## 25. Multi-Level Architecture (Tower of Quotients)

### 25.1 Theoretical Framework

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TOWER OF QUOTIENTS                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                         Level N: Discourse attractors
                              â†‘
                         Level 3: Sentence attractors
                              â†‘
                         Level 2: Phrase attractors
                              â†‘
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Level 1: Word attractors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                 â”‚
    â”‚   Tokens â†’ Cl(3,1) embeddings â†’ geometric product â†’ attractor  â”‚
    â”‚                                                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KEY INSIGHT: Each level is a complete Cl(3,1) system with its own:
    - Token embeddings
    - Witness (gauge-invariant anchor)
    - Grace contraction
    - Attractor storage

Attractors from Level N become TOKENS for Level N+1.
```

### 25.2 Implementation

**See `holographic/hierarchy.py` for the full implementation.**

```python
from holographic import HierarchicalModel

# Create 2-level model
model = HierarchicalModel(
    vocab_size=10000,
    num_levels=2,
    codebook_size=1000,  # L1 attractors â†’ L2 tokens
)

# Train Level 1
model.levels[0].associate(context, target_embedding)

# Build codebook: L1 attractors â†’ L2 tokens
model.update_codebook(level=1)

# Train Level 2 on phrase-level patterns
model.levels[1].associate(phrase_context, phrase_target)
```

### 25.3 Why This Architecture

| Feature | Single Cl(3,1) | Tower of Quotients |
|---------|----------------|-------------------|
| Local semantics | âœ“ | âœ“ |
| Short contexts | âœ“ | âœ“ |
| Long-range abstraction | âœ— | âœ“ |
| Polysemy | âœ— | âœ“ |
| Narrative identity | âœ— | âœ“ |

**The tower doesn't just ADD capacityâ€”it adds ABSTRACTION layers.**

### 25.4 The Missing Ingredient

**Problem**: Level 2 can't create semantics that Level 1 doesn't have.

**Required**: A learning signal that creates semantic structure at Level 1.

**Options under investigation**:
1. Contrastive learning (partially effective)
2. Semantic supervision (external signal)
3. Self-supervised structure discovery
4. Active Inference EFE minimization

---

## 26. Diagnostics Module

**See `holographic/diagnostics.py` for tools to understand model behavior.**

```python
from holographic import run_level1_diagnostics

results = run_level1_diagnostics(
    contexts, targets, embeddings, basis,
    verbose=True
)

# Returns:
# - semantic_coherence: same-target vs diff-target similarity
# - witness_stability: how stable is the self-pointer across contexts
# - grade_analysis: which grades differentiate, which stay stable
```

**Key Metric**: **Separation** = same_target_sim - diff_target_sim
- Positive separation â†’ learning semantic structure
- Near-zero separation â†’ learning co-occurrence only
- Negative separation â†’ worse than random

---

## 27. v3.0 Implementation Results: Compositional Pipeline

### 27.1 Architecture Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              COMPOSITIONAL HOLOGRAPHIC MODEL (v3.0)                         â”‚
â”‚                                                                             â”‚
â”‚   1. FEATURE SET: 14 orthogonal directions in Cl(3,1) grades 1-3           â”‚
â”‚       fâ‚...fâ‚„  âˆˆ Grade 1 (vectors)                                         â”‚
â”‚       fâ‚…...fâ‚â‚€ âˆˆ Grade 2 (bivectors)                                       â”‚
â”‚       fâ‚â‚..fâ‚â‚„ âˆˆ Grade 3 (trivectors)                                      â”‚
â”‚                                                                             â”‚
â”‚   2. WORD EMBEDDING:                                                        â”‚
â”‚       embed(word) = 0.3Â·I + Î£áµ¢ Î±áµ¢(word) Â· fáµ¢                               â”‚
â”‚       where Î±áµ¢ âˆˆ [0, Ï†â»Â¹] are per-word coefficients                        â”‚
â”‚                                                                             â”‚
â”‚   3. HEBBIAN LEARNING:                                                      â”‚
â”‚       When (context, target) co-occur:                                      â”‚
â”‚       Î”Î±áµ¢(context_word) âˆ Î±áµ¢(target) - Î±áµ¢(context_word)                   â”‚
â”‚       "Pull co-occurring words toward shared features"                      â”‚
â”‚                                                                             â”‚
â”‚   4. ONE-SHOT INFERENCE:                                                    â”‚
â”‚       New word in context â†’ features â‰ˆ average of context word features    â”‚
â”‚                                                                             â”‚
â”‚   5. RETRIEVAL:                                                             â”‚
â”‚       Novel context â†’ find most similar WORD embedding                      â”‚
â”‚       (not attractor storage)                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 27.2 Empirical Results

**Test: Structured semantic data (5 categories, 20 words each, 10,000 samples)**

| Embedding Type | Separation | Same-Target Sim | Diff-Target Sim |
|---------------|------------|-----------------|-----------------|
| Atomic (random) | 0.057 | 0.719 | 0.663 |
| **Compositional** | **0.717** | **1.000** | **0.283** |

**Improvement: 12.6x better semantic separation**

### 27.3 Generation Quality

| Context Type | Generated (Atomic) | Generated (Compositional) |
|-------------|-------------------|--------------------------|
| Animal [0,1,2,3,4] | [27,27,27...] (vehicles) âœ— | [18,18,18...] (animals) âœ“ |
| Vehicle [20,21,22...] | Random | Vehicles (10/10) âœ“ |

**Key Fix**: Novel contexts now decoded via word embedding similarity, not stored attractor indices.

### 27.4 One-Shot Learning

```
Test: Learn new word 99 from animal context [0,1,2,3,4]

Result:
    Similarity to animals:  0.998
    Similarity to vehicles: 0.873
    
âœ“ New word correctly clusters with its context category
```

### 27.5 Key Files

| File | Purpose |
|------|---------|
| `compositional.py` | `CompositionalEmbedding`, `FeatureSet` |
| `feature_learning.py` | `CooccurrenceTracker`, `learn_features_hebbian`, `one_shot_learn_word` |
| `full_pipeline.py` | `CompositionalHolographicModel` (integrated) |

### 27.6 Usage

```python
from holographic import CompositionalHolographicModel

# Create model
model = CompositionalHolographicModel(
    vocab_size=10000,
    num_features=14,
    context_size=5,
    max_attractors=50000,
)

# Train with Hebbian learning
model.train(contexts, targets, hebbian_lr=0.05, verbose=True)

# Generate from context
tokens = model.generate([1, 2, 3, 4, 5], num_tokens=10)

# One-shot learn new word from context
model.one_shot_learn(new_word_idx, context_list, strength=0.8)
```

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| **3.0.0** | 2026-01-08 | **Full compositional pipeline** - Hebbian + attractor + one-shot |
| 2.9.0 | 2026-01-08 | Compositional embeddings implementation |
| 2.8.0 | 2026-01-08 | Compositional embeddings insight (Section 23) |
| 2.7.0 | 2026-01-08 | Multi-level hierarchy, diagnostics, empirical findings |
| 2.6.0 | 2026-01-08 | Quotient structure, binding operator |
| 2.4.0 | 2026-01-08 | Cross-disciplinary foundations (Section 22) |
| 2.3.0 | 2026-01-08 | Topological foundations (FOUNDATIONS.md) |
| 2.2.0 | 2026-01-08 | Active Inference extension (EFE-based generation) |
| 2.1.0 | 2026-01-08 | Matrix representation Cl(3,1) â‰… Mâ‚„(â„) |
| 2.0.0 | 2026-01-07 | Hierarchical retrieval (deprecated) |
| 1.x | pre-2026 | Legacy 16D vector implementation (archived) |
